<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                 "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
  <title>The deal.II Testsuite</title>
  <link href="../screen.css" rel="StyleSheet">
  <meta name="copyright" content="Copyright (C) 1998, 1999, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014 by the deal.II Authors">
  <meta name="date" content="$Date$">
  <meta name="keywords" content="deal dealii finite elements fem triangulation">
  <meta http-equiv="content-language" content="en">
</head>
<body>


    <h1>The deal.II Testsuite</h1>

    <p>
      The deal.II testsuite consists of two parts:
      <i>build tests</i> and the
      <i>regression testsuite</i>. While the build tests
      are used to check that the
      library can be compiled on different systems and with different (versions
      of) compilers, the regression tests are actually run and their output
      compared with previously stored output files to verify that what
      worked yesterday still works today. These two testsuites are
      described below.
    </p>

    <p>
      deal.II has a testsuite that currently has (mid-2014), some 3,400
      small programs (growing by roughly one per day) that we run every
      time we make a change to make sure that no existing functionality is
      broken. The expected output for every test is stored in an output
      file, and when you run a test you are notified if a test produces
      different output. These days, every time we add a significant piece
      of functionality, we add at least one new test to the testsuite, and
      we also do so if we fix a bug, in both cases to make sure that future
      changes do not break what we have just checked in. Machines running
      tests send results back home and these are then converted into
      <a href="http://cdash.kyomu.43-1.org/index.php?project=deal.II"
      target="body">a webpage showing the status of our regression tests</a>.
    </p>

    <div class="toc">
      <ol>
        <li><a href="#quick">Quick instructions</a></li>
        <li><a href="#setup">Setting up the testsuite</a></li>
        <li><a href="#run">Running the testsuite</a></li>
        <ol>
          <li><a href="#runoutput">How to interpret the output</a></li>
          <li><a href="#runmakeopts">Parallelizing testsuite configuration</a></li>
          <li><a href="#coverage">Generating coverage information</a></li>
        </ol>
        <li><a href="#layout">Testsuite development</a></li>
        <ol>
          <li><a href="#layoutgeneral">General layout</a></li>
          <li><a href="#restrictcompiler">Restricting tests to specific compiler and compiler versions</a></li>
          <li><a href="#restrictbuild">Restricting tests to build configurations</a></li>
          <li><a href="#restrictfeature">Restricting tests to feature configurations</a></li>
          <li><a href="#mpi">Running tests with MPI</a></li>
          <li><a href="#binary">Tests with binary output</a></li>
          <li><a href="#expect">Changing condition for success</a></li>
          <li><a href="#layoutaddtests">Adding new tests</a></li>
          <li><a href="#layoutaddcategory">Adding new categories</a></li>
        </ol>
        <li><a href="#submit">Submitting test results</a></li>
        <li><a href="#build_tests">Build tests</a></li>
          <ol>
            <li><a href="#dedicatedbuilds">Dedicated build tests</a></li>
          </ol>
      </ol>
    </div>

    <a name="quick"></a>
    <h2>Quick instructions</h2>

    <p>
      If you're impatient, use the following commands:
<pre>
$ mkdir new_directory
$ cd new_directory
$ svn checkout https://svn.dealii.org/trunk .
$ mkdir build
$ cd build
$ cmake ../deal.II
$ make -j16
$ make -j16 setup_tests
$ ctest -j16
</pre>
      The exact meaning of all of these commands will be explained in much
      greater detail below.
    </p>

    <a name="setup"></a>
    <h2>Setting up the testsuite</h2>

    <p> In order to run it, you need to checkout the git repository with
    the development version of deal.II. If you haven't already done this
    clone the repository via:
<pre>
$ git clone https://github.com/dealii/dealii.git
</pre>
    More information about the git repository is available on the <a
      href=http://www.dealii.org/download.html>homepage</a>.
    </p>

    <p>
      To enable the testsuite, configure and build deal.II in a build
      directory as normal (installation is not necessary). After that you
      can setup the testsuite via the "setup_tests" target:s
<pre>
$ make setup_tests
</pre>
      This will set up all tests supported by the current configuration.
      The testsuite can now be run in the current <i>build directory</i> as
      described below.
    </p>

    <p>
      Setup can be fine-tuned using the following commands:
<pre>
$ make regen_tests - reruns configure stage in every testsuite subproject

$ make clean_tests - runs the 'clean' target in every testsuite subproject

$ make prune_tests - removes all testsuite subprojects
</pre>

    <p>
      In addition, when setting up the testsuite, the following environment
      variables can be used to override default behavior when
      calling <code>make setup_tests</code>:
<pre>
TEST_DIFF
  - The diff tool and command line to use for comparison. If numdiff is
    available it defaults to "numdiff -a 1e-6 -q", otherwise plain diff
    is used.

TEST_TIME_LIMIT
  - The time limit (in seconds) a single test is allowed to take. Defaults
    to 180 seconds

TEST_PICKUP_REGEX
  - A regular expression to select only a subset of tests during setup.
    An empty string is interpreted as a catchall (this is the default).
</pre>
    </p>

    <a name="run"></a>
    <h2>Running the testsuite</h2>

    <p>
      The testsuite can now be run in the <i>build directory</i> via
<pre>
$ ctest [-j N]
</pre>
      Here, <code>N</code> is the number of concurrent tests that should be
      run, in the same way as you can say <code>make -jN</code>. The testsuite
      is huge and will need around 12h on current computers 
      running single threaded.
    </p>

    <p>
      If you only want to run a subset of tests
      matching a regular expression, or if you want to exclude tests matching
      a regular expression, you can use
<pre>
$ ctest [-j N] -R '&lt;positive regular expression&gt;'
$ ctest [-j N] -E '&lt;negative regular expression&gt;'
</pre>
    </p>

    <p>
      <b>Note:</b>
      Not all tests succeed on every machine even if all computations are
      correct, because your machine generates slightly different floating
      point outputs. To increase the number of tests that work correctly,
      install the
      <a href="http://www.nongnu.org/numdiff/">numdiff</a> tool that compares
      stored and newly created output files based on floating point
      tolerances. To use it, simply export where the <code>numdiff</code>
      executable can be found via the <code>PATH</code> 
      environment variable so that it can be found during
      <code>make setup_tests</code>.
    </p>

    <a name="runoutput"></a>
    <h3>How to interpret the output</h3>

    <p>
      A typical output of a <code>ctest</code> invocation looks like:
<pre>
$ ctest -j4 -R "base/thread_validity"
Test project /tmp/trunk/build
      Start 747: base/thread_validity_01.debug
      Start 748: base/thread_validity_01.release
      Start 775: base/thread_validity_05.debug
      Start 776: base/thread_validity_05.release
 1/24 Test #776: base/thread_validity_05.release ...   Passed    1.89 sec
 2/24 Test #748: base/thread_validity_01.release ...   Passed    1.89 sec
      Start 839: base/thread_validity_03.debug
      Start 840: base/thread_validity_03.release
 3/24 Test #747: base/thread_validity_01.debug .....   Passed    2.68 sec
[...]
      Start 1077: base/thread_validity_08.debug
      Start 1078: base/thread_validity_08.release
16/24 Test #1078: base/thread_validity_08.release ...***Failed    2.86 sec
18/24 Test #1077: base/thread_validity_08.debug .....***Failed    3.97 sec
[...]

92% tests passed, 2 tests failed out of 24

Total Test time (real) =  20.43 sec

The following tests FAILED:
        1077 - base/thread_validity_08.debug (Failed)
        1078 - base/thread_validity_08.release (Failed)
Errors while running CTest
</pre>
      If a test failed (like <code>base/thread_validity_08.debug</code> in above
      example output), you might want to find out what exactly went wrong. To
      this end, you can search
      through <code>Testing/Temporary/LastTest.log</code> for the exact output
      of the test, or you can rerun this one test, specifying <code>-V</code>
      to select verbose output of tests:
<pre>
$ ctest -V -R "base/thread_validity_08.debug"
[...]
test 1077
    Start 1077: base/thread_validity_08.debug

1077: Test command: [...]
1077: Test timeout computed to be: 600
1077: Test base/thread_validity_08.debug: RUN
1077: ===============================   OUTPUT BEGIN  ===============================
1077: Built target thread_validity_08.debug
1077: Generating thread_validity_08.debug/output
1077: terminate called without an active exception
1077: /bin/sh: line 1: 18030 Aborted [...]/thread_validity_08.debug
1077: base/thread_validity_08.debug: BUILD successful.
1077: base/thread_validity_08.debug: RUN failed. Output:
1077: DEAL::OK.
1077: gmake[3]: *** [thread_validity_08.debug/output] Error 1
1077: gmake[2]: *** [CMakeFiles/thread_validity_08.debug.diff.dir/all] Error 2
1077: gmake[1]: *** [CMakeFiles/thread_validity_08.debug.diff.dir/rule] Error 2
1077: gmake: *** [thread_validity_08.debug.diff] Error 2
1077:
1077:
1077: base/thread_validity_08.debug: ******    RUN failed    *******
1077:
1077: ===============================    OUTPUT END   ===============================
</pre>
      So this specific test aborted in the <code>RUN</code> stage.
    </p>

    <p>
      The general output for a successful test <code>&lt;test&gt;</code> in
      category <code>&lt;category&gt;</code> for build type
      <code>&lt;build&gt;</code> is
<pre>
xx: Test &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: PASSED
xx: ===============================   OUTPUT BEGIN  ===============================
xx: [...]
xx: &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: PASSED.
xx: ===============================    OUTPUT END   ===============================
</pre>
      And for a test that fails in stage <code>&lt;stage&gt;</code>:
<pre>
xx: Test &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: &lt;stage&gt;
xx: ===============================   OUTPUT BEGIN  ===============================
xx: [...]
xx: &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: &lt;stage&gt; failed. [...]
xx:
xx: &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: ******    &lt;stage&gt; failed    *******
xx: ===============================    OUTPUT END   ===============================
</pre>
      Hereby, <code>&lt;stage&gt;</code> indicates the stage in which the
      test failed:
      <ul>
        <li>
          <code>CONFIGURE</code>: only for test in the "build_tests"
          category: The test project failed in the configuration stage
        </li>
        <li>
          <code>BUILD</code>: compilation error occured
        </li>
        <li>
          <code>RUN</code>: the test executable could not be run / aborted
        </li>
        <li>
          <code>DIFF</code>: the test output differs from the reference output
        </li>
        <li>
          <code>PASSED</code>: the test run successful
        </li>
      </ul>
      Typically, tests fail because the output has changed, and you will see
      this in the DIFF phase of the test.
    </p>


    <a name="runmakeopts"></a>
    <h3>Parallelizing testsuite configuration</h3>

    <p>
      If you have set up the testsuite (or at least a few hundred tests by
      providing a sufficiently general regular expression
      as <code>TEST_PICKUP_REGEX</code>) you'll notice that a 
      reconfiguration of the build directory takes a noticeable time (in the
      order of minutes instead of seconds) with
      <code>Reconfiguring testsuite subprojects</code>.
      By default (with the "Unix Makefiles" generator) this is due to the
      fact that the reconfiguration processes the subprojects sequentally.
      If you want to increase the parallelism, specify e.g.
      <code>MAKEOPTS="-j8"</code> as environment variable or as CMake
      variable.
    </p>
    <p>
      Alternatively, you can use <code>Ninja</code> as your build tool instead
      of <code>make</code>.
    </p>



    <a name="coverage"></a>
    <h3>Generating coverage information</h3>

    <p>
      The testsuite can also be used to provide <i>coverage</i> information,
      i.e., data that shows which lines of the library are executed how many
      times by running through all of the tests in the testsuite. This is of
      interest in finding places in the library that are not covered by
      the testsuite and, consequently, are prone to the inadvertent
      introduction of bugs since existing functionality is not subject to
      existing tests.
    </p>

    <p>
      To run the testsuite in this mode, essentially, you have to do three
      things: 
      <ol>
	<li>
	  build the library with appropriate profiling flags
	</li>
	<li>
	  run all or some tests (built with the same profiling flags)
	</li>
	<li>
	  gather all information and convert them to a viewable format.
	</li>
      </ol>
      In order to achieve the first two, configure the library with
      <pre>
  cmake -DCMAKE_BUILD_TYPE=Debug -DDEAL_II_SETUP_COVERAGE=YES <...>
      </pre>
      You can then build the library and run the tests as usual.
    </p>

    <p>
      For the last point, one can in principal use whatever tool one
      wants. That said, the deal.II ctest driver already has builtin
      functionality to gather all profiling files and submit them to cdash
      where we already gather testsuite results
      (see <a href="#submit">below</a>). You can do so by invoking
      <pre>
  ctest -DCOVERAGE=YES <...> -S ../tests/run_testsuite.cmake
      </pre>
      when running the testsuite, or directly by
      <pre>
	ctest <...> -S ../tests/run_coverage.cmake
      </pre>
    </p>

    <p>
      At the end of all of this, results will be shown in a separate section
      "Coverage" at the
      <a href="http://cdash.kyomu.43-1.org/index.php?project=deal.II&display=project"
      target="_top">deal.II cdash site</a>.
    </p>


    <a name="layout"></a>
    <h2>Testsuite development</h2>

    <p>
      The following outlines what you need to know if you want to understand
      how the testsuite actually works, for example because you may want to
      add tests along with the functionality you are currently developing.
    </p>



    <a name="layoutgeneral"></a>
    <h3>General layout</h3>

    <p>
      A test usually consists of a source file and an output file for
      comparison (under the testsuite directory <code>tests</code>):
<pre>
category/test.cc
category/test.output
</pre>
      <code>category</code> will be one of the existing subdirectory
      under <code>tests/</code>, e.g., <code>lac/</code>, <code>base/</code>,
      or <code>mpi/</code>. Historically, we have grouped tests into the
      directories <code>base/, lac/, deal.II/</code> depending on their
      functionality, and <code>bits/</code> if they were small unit tests, but
      in practice we have not always followed this rigidly. There are also
      more specialized directories <code>trilinos/, petsc/,
      serialization/, mpi/</code> etc, whose meaning is more obvious.
      <code>test.cc</code> must be a regular executable (i.e. having an
      <code>int main()</code> routine). It will be compiled, linked and
      run. The executable should not output anything to <code>cout</code>
      (at least under normal circumstances, i.e. no error condition),
      instead the executable should output to a file <code>output</code>
      in the current working directory. In practice, we rarely write the
      source files completely from scratch, but we find an existing test that
      already does something similar and copy/modify it to fit our needs.
    </p>
    <p>
      For a normal test, <code>ctest</code> will typically run the following 3
      stages:
      <ul>
        <li>
          <code>BUILD</code>: The build stage generates an executable in
          <code>BUILD_DIR/tests/&lt;category&gt;/&lt;test&gt;</code>.
        </li>
        <li>
          <code>RUN</code>: The run stage then invokes the executable in the
          directory where it is located. By convention, each test puts its
          output into a file simply called <code>output</code>, which will
          then be located in
          <code>BUILD_DIR/tests/&lt;category&gt;/&lt;test&gt;/output</code>.
          If the run fails (e.g. because the program aborts with an error
          code) the file <code>output</code> is renamed to
          <code>failing_output</code>.
        </li>
        <li>
          <code>DIFF</code>: As a last stage the generated output file will
          be compared to
          <code>SOURCE_DIR/tests/&lt;category&gt;/&lt;test&gt;[...].output</code>.
          and stored in
          <code>BUILD_DIR/tests/&lt;category&gt;/&lt;test&gt;/diff</code>.
          If the diff fails  the file <code>diff</code> is renamed to
          <code>failing_diff</code>.
        </li>
      </ul>
    </p>


    <a name="restrictbuild"></a>
    <h3>Restricting tests to build configurations</h3>

    <p>
      Comparison file can actually be named in a more complex way than
      just <code>category/test.output</code>:
<pre>
category/test.[compiler=&lt;regex&gt;=&lt;yes|no&gt;.]*[with_&lt;feature&gt;=&lt;on|off&gt;.]*[mpirun=&lt;x&gt;.][expect=&lt;y&gt;.][binary.][&lt;debug|release&gt;.]output
</pre>
      Normally, a test will be set up so that it runs twice, once in debug and
      once in release configuration.
      If a specific test can only be run in debug or release configurations but
      not in both it is possible to restrict the setup by prepeding
      <code>.debug</code> or <code>.release</code> directly before
      <code>.output</code>, e.g.:
<pre>
category/test.debug.output
</pre>
      This way, the test will only be set up to build and run against the debug
      library. If a test should run in both configurations but, for some
      reason, produces different output (e.g., because it triggers an
      assertion in debug mode), then you can just provide two different output
      files:
<pre>
category/test.debug.output
category/test.release.output
</pre>
    </p>

    <a name="restrictcompiler"></a>
    <h3>Restricting tests to specific compiler and compiler versions</h3>
    <p>
      In a similar vain as for build configurations, it is possible to restrict
      tests to specific compiler versions, e.g.:
<pre>
category/test.compiler=GNU=yes.output, or
category/test.compiler=ICC-14=no.output
</pre>
      These tests will only be set up if the specified regular expression
      matches (in case of <code>=yes</code>), or doesn't match
      (<code>=no</code>) the string
      <code>${CMAKE_CXX_COMPILER_ID}-${CMAKE_CXX_COMPILER_VERSION}</code>.
      Common compiler names are <code>GNU</code>, <code>Clang</code> or
      <code>Intel</code>.

    <a name="restrictfeature"></a>
    <h3>Restricting tests to feature configurations</h3>
    <p>
      In a similar vain as for build configurations, it is possible to restrict
      tests to specific feature configurations, e.g.:
<pre>
category/test.with_umfpack=on.output, or
category/test.with_zlib=off.output
</pre>
      These tests will only be set up if the specified feature was configured.
      It is possible to provide different output files for disabled/enabled
      features, e.g.
<pre>
category/test.with_64bit_indices=on.output
category/test.with_64bit_indices=off.output
</pre>
      It is also possible to declare multiple constraints subsequently, e.g.
<pre>
category/test.with_umfpack=on.with_zlib=on.output
</pre>
    </p>
    <p>
      <b>Note:</b> The tests in some subdirectories of <code>tests/</code> are
      automatically run only if some feature is enabled. In this case a
      feature constraint encoded in the output file name is 
      redundant and should be avoided. In particular, this holds for
      subdirectories 
      <code>distributed_grids</code>, <code>lapack</code>,
      <code>metis</code>, <code>petsc</code>, <code>slepc</code>,
      <code>trilinos</code>, <code>umfpack</code>, <code>gla</code>, and
      <code>mpi</code>
    </p>


    <a name="mpi"></a>
    <h3>Running tests with MPI</h3>
    <p>
      If a test should be run with MPI in parallel, the number of MPI
      processes <code>N</code> with which a program needs to be run for
      comparison with a given output file is specified as follows:
<pre>
category/test.mpirun=N.output
</pre>
      It is quite typical for an MPI-enabled test to have multiple output
      files for different numbers of MPI processes.
    </p>

    <a name="binary"></a>
    <h3>Tests with binary output</h3>
    <p>
      If a test produces binary output add <code>binary</code> to the
      output file to indicate this:
<pre>
category/test.binary.output
</pre>
      The testsuite ensures that a diff tool suitable for comparing binary
      output files is used instead of the default diff tool, which (as in
      the case of <code>numdiff</code>) might be unable to compare binary
      files.
    </p>

    <a name="expect"></a>
    <h3>Changing condition for success</h3>
    <p>
      Normally a test is considered to be successful if all test stages
      could be run and the test reached the <code>PASSED</code> stage (see
      the <a href="#runoutput">output description</a> section for details).
      If (for some reason) the test should succeed ending at a specific
      test stage different than <code>PASSED</code> you can specify it via
      <code>expect=&lt;stage&gt;</code>, e.g.:
<pre>
category/test.expect=run.output
</pre>
    </p>



    <a name="layoutaddtests"></a>
    <h3>Adding new tests</h3>

    <p>
      We typically add one or more new tests every
      time we add new functionality to the library or fix a bug. If you
      want to contribute code to the library, you should do this
      as well. Here's how: you need a testcase and a file with the
      expected output.
    </p>

    <h4>The testcase</h4>
    <p>
      For the testcase, we usually start from one of the existing tests, copy
      and modify it to where it does what we'd like to test. Alternatively,
      you can also start from a template like this:
<pre>
// ---------------------------------------------------------------------
//
// Copyright (C) 2013 by the deal.II Authors
//
// This file is part of the deal.II library.
//
// The deal.II library is free software; you can use it, redistribute
// it, and/or modify it under the terms of the GNU Lesser General
// Public License as published by the Free Software Foundation; either
// version 2.1 of the License, or (at your option) any later version.
// The full text of the license can be found in the file LICENSE at
// the top level of the deal.II distribution.
//
// ---------------------------------------------------------------------

// a short (a few lines) description of what the program does

#include "../tests.h"
#include &lt;iostream&gt;
#include &lt;fstream&gt;

// all include files you need here

int main ()
{
  std::ofstream logfile("output");
  deallog.attach(logfile);
  deallog.depth_console(0);

  // your testcode here:
  int i=0;
  deallog << i << std::endl;

  return 0;
}
</pre>

    <p>This code opens an output file <code>output</code> in the current working
    directory and then writes all output you generate to it, through the
    <code>deallog</code> stream. The <code>deallog</code> stream works like
    any other <code>std::ostream</code> except that it does a few more
    things behind the scenes that are helpful in this context. In above
    case, we only write a zero to the output file. Most tests of course
    write computed data to the output file to make sure that whatever we
    compute is what we got when the test was first written.
    </p>

    <p>
    There are a number of directories where you can put a new test.
    Extensive tests of individual classes or groups of classes
    have traditionally been into the <code>base/</code>,
    <code>lac/</code>, <code>deal.II/</code>, <code>fe/</code>,
    <code>hp/</code>, or <code>multigrid/</code> directories, depending on
    where the classes that are tested are located. More atomic tests often go
    into <code>bits/</code>. There are also
    directories for PETSc and Trilinos wrapper functionality.
    </p>

    <h4>An expected output</h4>

    <p>
      In order to run your new test, copy it to an appropriate category and
      create an empty comparison file for it:
<pre>
category/my_new_test.cc
category/my_new_test.output
</pre>
      Now, rerun
<pre>
$ make setup_tests
</pre>
      so that your new test is picked up. After that it is possible to
      invoke it with
<pre>
$ ctest -V -R "category/my_new_test"
</pre>
    </p>

    <p>
      If you run your new test executable this way, the test should compile
      and run successfully but fail in the diff stage (because of the empty
      comparison file). You will get an output file
      <code>BUILD_DIR/category/my_new_test/output</code>. Take a look at it to
      make sure that the output is what you had expected. (For complex tests,
      it may sometimes be impossible to say whether the output is correct, and
      in this case we sometimes just take it to make
      sure that future invokations of the test yield the same results.)
    </p>

    <p>
      The next step is to copy and rename this output file to the source
      directory and replace the original comparison file with it:
<pre>
category/my_new_test.output
</pre>
      At this point running the test again should be successful:
<pre>
$ ctest -V -R "category/my_new_test"
</pre>
    </p>


    <h4>Checking in</h4>

    <p>
      Tests are a way to make sure everything keeps working. If they aren't
      automated, they are no good. We are therefore very interested in
      getting new tests. Talk to us in the discussion group! Writing
      testcases is a worthy and laudable task, and we would like to
      encourage it by giving people the opportunity to contribute!
    </p>



    <a name="layoutaddcategory"></a>
    <h2>Adding new categories</h2>

    <p>
      If you want to create a new category in the testsuite, create an new
      folder under <cmake>./tests</cmake> that is named accordingly and put
      a <code>CMakeLists.txt</code> file into it containing
    </p>
<pre>
CMAKE_MINIMUM_REQUIRED(VERSION 2.8.8)
INCLUDE(${DEAL_II_SOURCE_DIR}/cmake/setup_testsubproject.cmake)
PROJECT(testsuite CXX)
INCLUDE(${DEAL_II_TARGET_CONFIG})
DEAL_II_PICKUP_TESTS()
</pre>



    <a name="submit"></a>
    <h2>Submitting test results</h2>

    <p>
      To submit test results to our <a
        href="http://cdash.kyomu.43-1.org/index.php?project=deal.II">CDash</a>
      instance just invoke ctest within a build directory (or designated
      build directory) with the <code>-S</code> option pointing to the
<pre>
$ ctest [...] -V -S ../tests/run_testsuite.cmake
</pre>
      The script will run configure, build and ctest and submit the results
      to the CDash server. It does not matter whether the configure, build
      or ctest stages were run before that. Also in script mode, you can
      specify the same options for <code>ctest</code> as explained above.
    </p>

    <p>
      <b>Note:</b> The default output in script mode is very minimal.
      Therefore, it is recommended to specify <code>-V</code> which will
      give the same level of verbosity as the non-script mode.
    </p>

    <p>
      <b>Note:</b> The following variables can be set to via
<pre>
ctest -D&lt;variable&gt;=&lt;value&gt; [...]
</pre>
      to control the behaviour of the <code>run_testsuite.cmake</code>
      script:
<pre>
CTEST_SOURCE_DIRECTORY
  - The source directory of deal.II (usually ending in "[...]/deal.II"
    (equivalent to https://svn.dealii.org/trunk/deal.II)
    Note: This is _not_ the test directory ending in "[...]/tests"
  - If unspecified, "../deal.II" and "../../" relative to the location
    of this script is used. If this is not a source directory, an error
    thrown.

CTEST_BINARY_DIRECTORY
  - The designated build directory (already configured, empty, or non
    existent - see the information about TRACKs what will happen)
  - If unspecified the current directory is used. If the current
    directory is equal to CTEST_SOURCE_DIRECTORY or the "tests"
    directory, an error is thrown.

CTEST_CMAKE_GENERATOR
  - The CMake Generator to use (e.g. "Unix Makefiles", or "Ninja", see
    $ man cmake)
  - If unspecified the current generator of a configured build directory
    will be used, otherwise "Unix Makefiles".

TRACK
  - The track the test should be submitted to. Defaults to "Experimental".
    Possible values are:

    "Experimental"     - all tests that are not specifically "build" tests
                         should go into this track

    "Build Tests"      - Build tests that configure and build in a clean
                         directory and run exactly all build tests matching
                         "build_tests/.*"

    "Regression Tests" - Reserved for the regression tester

    "Continuous"       - Reserved for the regression tester

CONFIG_FILE
  - A configuration file (see docs/development/Config.sample)
    that will be used during the configuration stage (invokes
    $ cmake -C ${CONFIG_FILE}). This only has an effect if
    CTEST_BINARY_DIRECTORY is empty.

DESCRIPTION
  - A string that is appended to CTEST_BUILD_NAME

COVERAGE
  - If set to TRUE deal.II will be configured with
    DEAL_II_SETUP_COVERAGE=TRUE, CMAKE_BUILD_TYPE=Debug and the
    CTEST_COVERAGE() stage will be run. Test results must go into the
    "Experimental" section.

MAKEOPTS
  - Additional options that will be passed directly to make (or ninja).
</pre>
      Furthermore, the variables described <a href="#setupconfigure">above</a> can also be
      set and will be handed automatically down to <code>cmake</code>.
    </p>



    <a name="build_tests"></a>
    <h2>Build tests</h2>

    <p>
      Build tests are used to check that deal.II can be compiled on
      different systems and with different compilers as well as
      different configuration options. Results are collected in the "Build
      Tests" track in <a
      href="http://cdash.kyomu.43-1.org/index.php?project=deal.II">CDash</a>.<p>

      <p>Running the build test suite is simple and we encourage deal.II
      users with configurations not found on the <a
      href="http://cdash.kyomu.43-1.org/index.php?project=deal.II">CDash page</a> to
      participate. Assuming you checked out deal.II into the directory
      <code>deal.II</code>, running it is as simple as:
<pre>
cd deal.II
mkdir build
cd build
ctest -j4 -S ../cmake/scripts/run_buildtest.cmake
</pre>
    </p>

    <p>
      What this does is to compile and build deal.II in the directory
      <code>build</code>, try to configure, build (and run a subset) of all
      tutorial programs supported by the current configuration and send the
      results to the CDash instance.
    </p>

    <p>
      <b>Note:</b> Build tests require the designated build directory to be
      completely empty If you want to specify a build configuration for
      cmake use a <a href="../users/Config.sample">configuration file</a>
      to preseed the cache as explained <a href="#submit">above</a>:
<pre>
$ ctest -DCONFIG_FILE="[...]/Config.sample" [...]
</pre>
    </p>



    <a name="dedicatedbuilds"></a>
    <h3>Dedicated build tests</h3>

    <p>
      Build tests work best if they run automatically and periodically.
      There is a detailed example for such dedicated build tests on the <a
        href="https://code.google.com/p/dealii/wiki/BuildTests">wiki</a>.
    </p>


    <hr />
    <address>
      <a href="http://www.dealii.org/authors.html" target="_top">The deal.II Authors</a>
      $Date$
    </address>
    <div class="right">
      <a href="http://validator.w3.org/check?uri=referer" target="_top">
        <img style="border:0" src="http://www.w3.org/Icons/valid-html401" alt="Valid HTML 4.01!"></a>
      <a href="http://jigsaw.w3.org/css-validator/check/referer" target="_top">
        <img style="border:0;width:88px;height:31px" src="http://jigsaw.w3.org/css-validator/images/vcss" alt="Valid CSS!"></a>
    </div>

  </body>
</html>
