<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                 "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
  <link href="../screen.css" rel="StyleSheet">
  <title>The deal.II Testsuite</title>
  <meta name="author" content="the deal.II authors <authors@dealii.org>">
  <meta name="copyright" content="Copyright (C) 1998, 1999, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009 by the deal.II authors">
  <meta name="date" content="$Date$">
  <meta name="svn_id" content="$Id$">
  <meta name="keywords" content="deal dealii finite elements fem triangulation">
  <meta http-equiv="content-language" content="en">
</head>
<body>


    <h1>The deal.II Testsuite</h1>
    
    <p>The deal.II testsuite consists of two parts, the 
    <a href="#build_tests">build tests</a> and the
    <a href="#regression_tests">regression tests</a>. While the build tests
    just check if the 
    library can be compiled on different systems and with different (versions
    of) compilers, the regression tests are actually run and their output
    compared with previously stored. These two testsuites are
    described below.</p>

    <a name="build_tests"></a>
    <h2>The build tests</h2>

    <p>With our build tests, we check if deal.II can be compiled on
    different systems and with different compilers as well as
    different configuration options. Results are collected in a
    database and can be accessed <a
    href="http://www.dealii.org/testsuite.html">online</a>.<p>

    <p>Running the build test suite is simple and we encourage deal.II
    users with configurations not found on the <a
    href="http://www.dealii.org/testsuite.html">test suite page</a> to
    participate. Assuming you checked out deal.II into the directory
    <code>dealtest</code>, running it is as simple as (you may want to pass
    different options than the ones given as <code>BUILDTESTFLAGS</code> to
    <code>./configure</code>):</p>

    <pre>

    cd dealtest
    make clean
    svn update
    make build-test BUILDTESTFLAGS='-with-umfpack -with-lapack'
    mail build-tests@dealii.org < build-test-log

    </pre>

    <p>After cleaning up the directory and updating the library to the
    most recent version, the <code>make</code> command configures the
    library (here for use of UMFPack and LAPACK libraries) and
    compiles everything. The result is in the file
    <code>build-test-log</code>, which is sent to the build test demon
    in the end. A status indicator should appear on the build test website
    after some time (results are collected and processed by a program that is
    run periodically, but not immediately after a mail has been received).
    </p>
    
    <h3>Configuring for a special compiler</h3>
    
    <p>If your compiler is not in the standard path or has a strange
    name (like <code>mycc</code> for C and <code>myCC</code> for C++),
    your template would be something like this (assuming you work with the
    <code>bash</code> shell):</p>

    <pre>

    export PATH=/my/compiler/path:$PATH
    export CXX=myCC

    cd dealtest
    make clean
    svn update
    make build-test BUILDTESTFLAGS='CC=mycc CXX=myCC'
    mail build-tests@dealii.org < build-test-log

    </pre>


    <h3>Dedicated tests</h3>

    <p>
    Most of our tests are built in dedicated directories, i.e. work on copies
    of deal.II that we don't usually use to work on every day. We do this,
    because we want to see if the version in the subversion repository can be
    compiled, not the version that we have made changes in.
    </p>

    <p>
    In this case, one would check out a new version, and do essentially the
    same things as above:

    <pre>

    export PATH=/my/compiler/path:$PATH
    export CXX=myCC

    svn co http://www.dealii.org/svn/dealii/trunk/deal.II
    cd deal.II
    svn update
    make build-test BUILDTEST=yes BUILDTESTFLAGS='CC=mycc CXX=myCC'
    mail build-tests@dealii.org < build-test-log
    cd ..
    rm -rf deal.II

    </pre>
    
    The only difference is that one has to give the option
    <code>BUILDTEST=yes</code> to the call to <code>make</code>. This is so
    because for the make file to work it depends on a file that is created by
    <code>./configure</code>; the latter, however, hasn't run on this pristine
    copy yet.
    </p>


    <a name="regression_tests"></a>
    <h2>The regression tests</h2>

    <p>
    deal.II has a testsuite that, at the time this article is written,
    has some 1650 small programs that we run every time we make a
    change to make sure that no existing functionality is broken. The
    expected output is also stored in our subversion archive, and when you
    run a test you are notified if a test fails. These days, every
    time we add a significant piece of functionality, we add at least
    one new test to the testsuite, and we also do so if we fix a bug,
    in both cases to make sure that future changes do not break what
    we have just checked in. In addition, some machines run the tests
    every night and send the results back home; this is then converted
    into <a href="http://www.dealii.org/testsuite.html"
    target="body">a webpage showing the status of our regression
    tests</a>.
    </p>

    <p>
    If you develop parts of deal.II, want to add something, or fix a
    bug in it, we encourage you to use our testsuite. This page
    documents some aspects of it.
    </p>


    <h3>Running it</h3>

    <p>
    To run the testsuite, go into your deal.II directory and do this:
    <pre>
       cd /path/to/deal.II
       svn checkout http://www.dealii.org/svn/dealii/trunk/tests
    </pre>
    This should generate a <code>tests/</code> directory parallel to
    the <code>base/</code>, <code>lac/</code>, etc directories. Then
    do this:
    <pre>
       cd tests
       ./configure
    </pre>
    This assumes that you have previously configured your deal.II
    installation and sets up a few things, in particular it determines
    which system you are on and which compiler you are using, in order
    to make sure that the stored correct output for your system and
    compiler is used to compare against the output of the tests when
    you run them.
    </p>

    <p>
    Once you have done this, you may simply type
    <code>make</code>. This runs all the tests there are, but stops at
    the first one that fails to either execute properly or for which
    the output does not match the expected output found in the subversion
    archive. This is helpful if you want to figure out if any test is
    failing at all. Typical output looks like this:
    <pre>
      deal.II/tests> make
      cd base ; make
      make[1]: Entering directory `/ices/bangerth/p/deal.II/1/deal.II/tests/base'
      =====linking======= logtest.exe
      =====Running======= logtest.exe
      =====Checking====== logtest.output
      =====OK============ logtest.OK
      =====linking======= reference.exe
      =====Running======= reference.exe
      =====Checking====== reference.output
      =====OK============ reference.OK
      =====linking======= quadrature_test.exe
      ...
    </pre>
    You should be aware that because of the number of tests we have,
    running the entire testsuite can easily take an hour, even on a
    fast system. On the other hand, only a large testsuite can offer
    comprehensive coverage of a software as big as deal.II.
    </p>

    <p>
    Sometimes, you know that for whatever reason one test
    always fails on your system, or has already failed before you made
    any changes to the library that could have caused tests to
    fail. We also sometimes check in tests that we know presently
    fail, just to remind us that we need to work on a fix, if we don't
    have the time to debug the problem properly right away. In this
    case, you will not want the testsuite to stop at the first test
    that fails, but will want to run all tests first and then inspect
    the output to find any fails. There are make targets for this
    as well. The usual way we use the testsuite is to run all tests
    like so:
    <pre>
      deal.II/tests> make report | tee report
      =======Report: base =======
      make[1]: Entering directory `/ices/bangerth/p/deal.II/1/deal.II/tests/base'
      2005-03-10 21:58 + anisotropic_1
      2005-03-10 21:58 + anisotropic_2
      2005-03-10 21:58 + auto_derivative_function
      2005-03-10 21:58 + data_out_base
      2005-03-10 21:58 + hierarchical
      2005-03-10 21:58 + logtest
      2005-03-10 21:58 + polynomial1d
      2005-03-10 21:58 + polynomial_test
      2005-03-10 21:58 + quadrature_selector
      ...
    </pre>
    This generates a report (that we "tee" into a file called "report"
    and show on screen at the same time). It shows the time at which
    the tests was run, an indicator of success, and the name of a
    test. The indicator is either a plus, which means that the test
    compiled and linked successfully and that the output compared
    successfully against the stored results. Or it is a minus,
    indicating that the test failed, which could mean that it didn't
    compile, it didn't link, or the results were wrong. Since it is
    often hard to see visually which tests have a minus (we should
    have used a capital X instead), this command
    <pre>
      grep " - " report
    </pre>
    picks out the lines that have a minus surrounded by spaces.
    </p>

    <p>
    If you want to do a little more than just that, you should
    consider running
    <pre>
      make report+mail | tee report
    </pre>
    instead. This does all the same stuff, but also mails the test
    result to our central mail result server which will in regular
    intervals (at least once a day) munge these mails and present them
    on our <a href="http://www.dealii.org/testsuite.html"
    target="body">test site</a>. This way, people can
    get an overview of what tests fail. You may even consider running
    tests nightly through a cron-job with this command, to have
    regular test runs.
    </p>

    <p>
    If a test failed, you have to find out what exactly went
    wrong. For this, you will want to go into the directory of that
    test, and figure out in more detail what went wrong. For example,
    if above test <code>hierarchical</code> would have failed, you
    would want to go into the <code>base</code> directory (this is
    given in the line with the equals signs; there are tests in other
    directories as well) and then type
    <pre>
      make hierarchical/exe
    </pre>
    to compile and link the executable. (For each test there is a not
    only a file with suffic <code>.cc</code> but also a subdirectory with the
    same name, in which we store among other things the executable for that
    test, under the name <code>exe</code>.) If this fails, i.e. if
    you can't compile or link, then you probably already know where
    the problem is, and how to fix it. If you could compile and link
    the test, you will want to make sure that it executes correctly
    and produces an output file:
    <pre>
      make hierarchical/output
    </pre>
    (As you see, the output file is also stored in the subdirectory with the
    test's name.) If this produces errors or triggers assertions, then you will
    want to use a debugger on the executable to figure out what happens. On
    the other hand, if you are sure that this also worked, you will
    want to compare the output with the stored output from subversion:
    <pre>
      make hierarchical/OK
    </pre>
    If the output isn't equal, then you'll see something like
    this:
    <pre>
      =====Checking====== hierarchical/output
      +++++Error+++++++++ hierarchical/OK. Use make verbose=on for the diffs
    </pre>
    Because the diffs between the output we get and the output we
    expected can sometimes be very large, you don't get to see it by
    default. However, following the suggestion printed, if you type
    <pre>
      make hierarchical/OK verbose=on
    </pre>
    you get to see it all:
    <pre>
      =====Checking====== hierarchical/output
      12c12
      < DEAL::0.333 1.667 0.333 -0.889 0.296 -0.988 0.329 -0.999 0.333 -1.000 0.333 -1.000
      ---
      > DEAL::0.333 0.667 0.333 -0.889 0.296 -0.988 0.329 -0.999 0.333 -1.000 0.333 -1.000
      +++++Error+++++++++ hierarchical/OK
    </pre>
    In this case, the second number on line 12 is off by one. To find
    the reason for this, you again should use a debugger or other
    suitable means, but that of course depends on what changes you
    have made last and that could have caused this discrepancy.
    </p>



    <h3>Adding new tests</h3>

    <p>
    As mentioned above, we add a new test every
    time we add new functionality to the library or fix a bug. If you
    want to contribute code to the library, you should do this
    as well. Here's how: you need a testcase, 
    a subdirectory with the same name as the test, an entry in the
    Makefile, and an expected output.
    </p>

    <h4>The testcase</h4>
    <p>
    For the testcase, we usually start from a template like this:
    <pre>
//----------------------------  my_new_test.cc  ---------------------------
//    $Id$
//    Version: $Name$ 
//
//    Copyright (C) 2005 by the deal.II authors 
//
//    This file is subject to QPL and may not be  distributed
//    without copyright and license information. Please refer
//    to the file deal.II/doc/license.html for the  text  and
//    further information on this license.
//
//----------------------------  my_new_test.cc  ---------------------------


// a short (a few lines) description of what the program does

#include "../tests.h"
#include <iostream>
#include <fstream>

// all include files you need here


int main () 
{
  std::ofstream logfile("my_new_test/output");
  deallog.attach(logfile);
  deallog.depth_console(0);

  // your testcode here:
  int i=0;
  deallog << i << std::endl;

  return 0;
}
    </pre>

    <p>You open an output file in a directory with the same
    name as your test, and then write
    all output you generate to it, 
    through the <code>deallog</code> stream.   The <code>deallog</code>
    stream works like any
    other <code>std::ostream</code> except that it does a few more
    things behind the scenes that are helpful in this context. In
    above case, we only write a zero to the output
    file. Most tests actually write computed data to the output file
    to make sure that whatever we compute is what we got when the
    test was first written.
    </p>

    <p>
    There are a number of directories where you can put a new test.
    Extensive tests of individual classes or groups of classes
    have traditionally been into the <code>base/</code>,
    <code>lac/</code>, <code>deal.II/</code>, <code>fe/</code>, 
    <code>hp/</code>, or <code>multigrid/</code> directories, depending on
    where the classes that are tested are located.
    </p>

    <p>
    We have started to create more atomic tests which
    are usually very small and test only a single aspect of the
    library, often only a single function. These tests go into the
    <code>bits/</code> directory and often have names that are
    composed of the name of the class being tested and a two-digit
    number, e.g., <code>dof_tools_11</code>. There are
    directories for PETSc and Trilinos wrapper functionality.
    </p>

    <h4>A directory with the same name as the test</h4>

    <p> You have to create a subdirectory
    with the same name as your test to hold the output from the test.

    <p> One convenient way to create this subdirectory with the correct
    properties is to use svn copy.
    <pre>
    svn copy existing_test_directory my_new_test
    </pre>

    <h4>An entry in the Makefile</h4>

    <p>
    In order for the Makefiles to pick up your new test, you have to
    add it there. In all the directories under <code>tests/</code>
    where tests reside, there is a separate Makefile that contains a
    list of the tests to be run in a variable called
    <code>tests_x</code>. You should add your test to the bottom of
    this list, by adding the base name of your testfile, i.e., without
    the extension <code>.cc</code>.  The entries can contain
    wildcards: for example, in the <code>tests/bits/</code> directory,
    the <code>tests_x</code> variable contains the entry
    <code>petsc_*</code> which at the time of this writing tests 120
    different tests that all match this pattern.
    </p>

    <p>
    If you have done this, you can try to run
    <pre>
      make my_new_test/output
    </pre>
    This should compile, link, and run your test. Running your test
    should generate the desired output file.
    </p>



    <h4>An expected output</h4>

    <p>
    If you run your new test executable, you will get an output file
    <code>mytestname/output</code> that should be used to compare all future
    runs with. If the test 
    is relatively simple, it is often a good idea to look at the
    output and make sure that the output is actually what you had
    expected. However, if you do complex operations, this may
    sometimes be impossible, and in this case we are quite happy with
    any reasonable output file just to make sure that future
    invokations of the test yield the same results.
    </p>

    <p>
    The next step is to copy this output file to the place where the
    scripts can find it when they compare with newer runs. For this, you first
    have to understand how correct results are verified. It works in the
    following way: for each test, we have subdirectories
    <code>testname/cmp</code> where we store the expected results in a file
    <code>testname/cmp/generic</code>. If you create a new test, you should
    therefore create this directory, and copy the output of your program, 
    <code>testname/output</code> to <code>testname/cmp/generic</code>.
    </p>

    <p>
    Why <code>generic</code>? The reason is that sometimes test results
    differ slightly from platform to platform, for example because numerical
    roundoff is different due to different floating point implementations on
    different CPUs. What this means is that sometimes a single stored output is
    not enough to verify that a test functioned properly: if you happen to be
    on a platform different from the one on which the generic output was
    created, your test will always fail even though it produces almost exactly
    the same output.
    </p>

    <p>
    To avoid this, what the makefiles do is to first check whether an output
    file is stored for this test and your particular configuration (platform
    and compiler). If this isn't the case, it goes through a hierarchy of files
    with related configurations, and only if none of them does it take the
    generic output file. It then compares the output of your test run with the
    first file it found in this process. To make things a bit clearer, if you
    are, for example, on a <code>i686-pc-linux-gnu</code> box and use
    <code>gcc4.0</code> as your compiler, then the following files will be
    sought (in this order): 
    <pre>
testname/cmp/i686-pc-linux-gnu+gcc4.0 
testname/cmp/i686-pc-linux-gnu+gcc3.4 
testname/cmp/i686-pc-linux-gnu+gcc3.3
testname/cmp/generic
    </pre>
    (This list is generated by the <code>tests/hierarchy.pl</code> script.)
    Your output will then be compared with the first one that is actually
    found. The virtue of this is that we don't have to store the output files
    from all possible platforms (this would amount to gigabytes of data), but
    that we only have store an output file for gcc4.0 if it differs from that
    of gcc3.4, and for gcc3.4 if it differs from gcc3.3. If all of them are the
    same, we would only have the generic output file.
    </p>

    <p>
    Most of the time, you will be able to generate output files only
    for your own platform and compiler, and that's alright: someone
    else will create the output files for other platforms
    eventually. You only have to copy your output file to
    <code>testname/cmp/generic</code>. 
    </p>

    <p>
    At this point you can run
    <pre>
      make my_new_test/OK
    </pre>
    which should compare the present output with what you have just
    copied into the compare directory. This should, of course,
    succeed, since the two files should be identical.
    </p>

    <p>
    On the other hand, if you realize that an existing test fails on your
    system, but that the differences (as shown when running with
    <code>verbose=on</code>, see above) are only marginal and around the 6th or
    8th digit, then you should check in your output file for the platform you
    work on. For this, you could copy <code>testname/output</code> to
    <code>testname/cmp/myplatform+compiler</code>, but your life can be easier
    if you simply type
    <pre>
      make my_new_test/ref
    </pre>
    which takes your output and copies it to the right place automatically.
    </p>




    <h4>Checking in</h4>
    
    <p>
    Tests are a way to make sure everything keeps working. If they
    aren't automated, they are no good. We are therefore very
    interested in getting new tests. If you have subversion write access
    already, you have to add the new test and the expected output
    file, and to commit them together with the changed Makefile, like
    so:
    <pre>
      svn add bits/my_new_test.cc 
      svn add bits/my_new_test
      svn add bits/my_new_test/cmp
      svn add bits/my_new_test/cmp/generic
      svn commit -m "New test" bits/my_new_test* bits/Makefile
    </pre>
    In addition, you should do the following in order to avoid that the files
    generated while running the testsuite show up in the output of <code>svn
    status</code> commands:
    <pre>
      svn propset svn:ignore "obj.*
        exe
        output
        status
        OK" bits/my_new_test
      svn commit -m "Ignore generated files." bits/my_new_test
    </pre>
    Note that the list of files given in quotes to the propset command extends
    over several lines.
    </p>

    <p>
    If you don't have subversion write access, talk to us on the mailing
    list; writing testcases is a worthy and laudable task, and we would
    like to encourage it by giving people the opportunity to
    contribute!
    </p>

    
    <address>
      <a href="../mail.html">The deal.II mailing list</a></address>
<div class="right">
  <p>
    <a href="http://validator.w3.org/check?uri=referer"><img border="0"
        src="http://www.w3.org/Icons/valid-html401"
        alt="Valid HTML 4.01!" height="31" width="88"></a>
  </p>
</div>

  </body>
</html>

