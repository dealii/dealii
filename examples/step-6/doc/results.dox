<h1>Results</h1>


The output of the program looks as follows:
@code
Cycle 0:
   Number of active cells:       20
   Number of degrees of freedom: 89
Cycle 1:
   Number of active cells:       44
   Number of degrees of freedom: 209
Cycle 2:
   Number of active cells:       92
   Number of degrees of freedom: 449
Cycle 3:
   Number of active cells:       200
   Number of degrees of freedom: 921
Cycle 4:
   Number of active cells:       440
   Number of degrees of freedom: 2017
Cycle 5:
   Number of active cells:       956
   Number of degrees of freedom: 4425
Cycle 6:
   Number of active cells:       1916
   Number of degrees of freedom: 8993
Cycle 7:
   Number of active cells:       3860
   Number of degrees of freedom: 18353
@endcode



As intended, the number of cells roughly doubles in each cycle. The
number of degrees is slightly more than four times the number of
cells; one would expect a factor of exactly four in two spatial
dimensions on an infinite grid (since the spacing between the degrees
of freedom is half the cell width: one additional degree of freedom on
each edge and one in the middle of each cell), but it is larger than
that factor due to the finite size of the mesh and due to additional
degrees of freedom which are introduced by hanging nodes and local
refinement.



The final solution, as written by the program at the end of the
<code>%run()</code> function, looks as follows:



<img src="https://www.dealii.org/images/steps/developer/step-6.solution.png" alt="">



In each cycle, the program furthermore writes the grid in EPS
format. These are shown in the following:



<TABLE WIDTH="100%">
<tr>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.grid-0.png" alt="">
</td>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.grid-1.png" alt="">
</td>
</tr>

<tr>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.grid-2.png" alt="">
</td>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.grid-3.png" alt="">
</td>
</tr>

<tr>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.grid-4.png" alt="">
</td>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.grid-5.png" alt="">
</td>
</tr>

<tr>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.grid-6.png" alt="">
</td>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.grid-7.png" alt="">
</td>
</tr>
</table>



It is clearly visible that the region where the solution has a kink,
i.e. the circle at radial distance 0.5 from the center, is
refined most. Furthermore, the central region where the solution is
very smooth and almost flat, is almost not refined at all, but this
results from the fact that we did not take into account that the
coefficient is large there. The region outside is refined rather
randomly, since the second derivative is constant there and refinement
is therefore mostly based on the size of the cells and their deviation
from the optimal square.



For completeness, we show what happens if the code we commented about
in the destructor of the <code>Step6</code> class is omitted
from this example.

@code
--------------------------------------------------------
An error occurred in line <128> of file <source/base/subscriptor.cc> in function
    void dealii::Subscriptor::check_no_subscribers() const
The violated condition was:
    counter == 0
The name and call sequence of the exception was:
    ExcInUse (counter, object_info->name(), infostring)
Additional Information:
Object of class N6dealii4FE_QILi2ELi2EEE is still used by 1 other objects.

(Additional information: <none>)

See the entry in the Frequently Asked Questions of deal.II (linked to from
http://www.dealii.org/) for a lot more information on what this error means and
how to fix programs in which it happens.

Stacktrace:
-----------
#0  /lib/libdeal_II.g.so: dealii::Subscriptor::check_no_subscribers() const
#1  /lib/libdeal_II.g.so: dealii::Subscriptor::~Subscriptor()
#2  /lib/libdeal_II.g.so: dealii::FiniteElement<2, 2>::~FiniteElement()
#3  ./step-6: Step6<2>::~Step6()
#4  ./step-6: main
--------------------------------------------------------
@endcode

From the above error message, we conclude that something is still using an
object with type <code>N6dealii4FE_QILi2EE</code>. This is of course the <a
href="http://en.wikipedia.org/wiki/Name_mangling">"mangled" name</a> for
<code>FE_Q</code>. The mangling works as follows: the <code>N6</code> indicates
a namespace with six characters (i.e., <code>dealii</code>) and the
<code>4</code> indicates the number of characters of the template class (i.e.,
<code>FE_Q</code>).

The rest of the text is then template arguments. From this we can already glean
a little bit who's the culprit here, and who the victim: The one object that
still uses the finite element is the <code>dof_handler</code> object.

The stacktrace gives an indication of where the problem happened. We
see that the exception was triggered in the
destructor of the <code>FiniteElement</code> class that was called
through a few more functions from the destructor of the
<code>Step6</code> class, exactly where we have commented out
the call to <code>DoFHandler::clear()</code>.



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

<h4>Solvers and preconditioners</h4>


One thing that is always worth playing around with if one solves
problems of appreciable size (much bigger than the one we have here)
is to try different solvers or preconditioners. In the current case,
the linear system is symmetric and positive definite, which makes the
CG algorithm pretty much the canonical choice for solving. However,
the SSOR preconditioner we use in the <code>solve()</code> function is
up for grabs.

In deal.II, it is relatively simple to change the preconditioner. For
example, by changing the existing lines of code
@code
  PreconditionSSOR<> preconditioner;
  preconditioner.initialize(system_matrix, 1.2);
@endcode
into
@code
  PreconditionSSOR<> preconditioner;
  preconditioner.initialize(system_matrix, 1.0);
@endcode
we can try out different relaxation parameters for SSOR. By using
(you have to also add the header file <code>lac/sparse_ilu.h</code> to
the include list at the top of the file)
@code
  PreconditionJacobi<> preconditioner;
  preconditioner.initialize(system_matrix);
@endcode
we can use Jacobi as a preconditioner. And by using
@code
  SparseILU<double> preconditioner;
  preconditioner.initialize(system_matrix);
@endcode
we can use a very simply incomplete LU decomposition without any
thresholding or strengthening of the diagonal.

Using these various different preconditioners, we can compare the
number of CG iterations needed (available through the
<code>solver_control.last_step()</code> call, see
step-4) as well as CPU time needed (using the Timer class,
discussed, for example, in step-12) and get the
following results (left: iterations; right: CPU time):

<TABLE WIDTH="60%" ALIGN="center">
  <tr>
    <td ALIGN="center">
      <img src="https://www.dealii.org/images/steps/developer/step-6.q2.dofs_vs_iterations.png" alt="">
    </td>

    <td ALIGN="center">
      <img src="https://www.dealii.org/images/steps/developer/step-6.q2.dofs_vs_time.png" alt="">
    </td>
  </tr>
</table>

As we can see, all preconditioners behave pretty much the same on this
simple problem, with the number of iterations growing like ${\cal
O}(N^{1/2})$ and because each iteration requires around ${\cal
O}(N)$ operations the total CPU time grows like ${\cal
O}(N^{3/2})$ (for the few smallest meshes, the CPU time is so small
that it doesn't record). Note that even though it is the simplest
method, Jacobi is the fastest for this problem. 

The situation changes slightly when the finite element is not a
bi-quadratic one as set in the constructor of this program, but a
bi-linear one. If one makes this change, the results are as follows:

<TABLE WIDTH="60%" ALIGN="center">
  <tr>
    <td ALIGN="center">
      <img src="https://www.dealii.org/images/steps/developer/step-6.q1.dofs_vs_iterations.png" alt="">
    </td>

    <td ALIGN="center">
      <img src="https://www.dealii.org/images/steps/developer/step-6.q1.dofs_vs_time.png" alt="">
    </td>
  </tr>
</table>

In other words, while the increase in iterations and CPU time is as
before, Jacobi is now the method that requires the most iterations; it
is still the fastest one, however, owing to the simplicity of the
operations it has to perform. This is not to say that Jacobi
is actually a good preconditioner -- for problems of appreciable size, it is
definitely not, and other methods will be substantially better -- but really
only that it is fast because its implementation is so simple that it can
compensate for a larger number of iterations. 

The message to take away from this is not that simplicity in
preconditioners is always best. While this may be true for the current
problem, it definitely is not once we move to more complicated
problems (elasticity or Stokes, for examples step-8 or
step-22). Secondly, all of these preconditioners still
lead to an increase in the number of iterations as the number $N$ of
degrees of freedom grows, for example ${\cal O}(N^\alpha)$; this, in
turn, leads to a total growth in effort as ${\cal O}(N^{1+\alpha})$
since each iteration takes ${\cal O}(N)$ work. This behavior is
undesirable: we would really like to solve linear systems with $N$
unknowns in a total of ${\cal O}(N)$ work; there is a class
of preconditioners that can achieve this, namely geometric (step-16,
step-37, step-39)
or algebraic multigrid (step-31, step-40, and several others)
preconditioners. They are, however, significantly more complex than
the preconditioners outlined above. 

Finally, the last message to take
home is that when the data shown above was generated (in 2008), linear
systems with 100,000 unknowns are
easily solved on a desktop machine in well under 10 seconds, making
the solution of relatively simple 2d problems even to very high
accuracy not that big a task as it used to be even in the recent
past. At the time, the situation for 3d problems was entirely different,
but even that has changed substantially in the intervening time -- though
solving problems in 3d to high accuracy remains a challenge.


<h4>A better mesh</h4>

If you look at the meshes above, you will see even though the domain is the 
unit disk, and the jump in the coefficient lies along a circle, the cells 
that make up the mesh do not track this geometry well. The reason, already hinted
at in step-1, is that by default the Triangulation class only sees a bunch of
coarse grid cells but has, of course, no real idea what kind of geometry they
might represent when looked at together. For this reason, we need to tell
the Triangulation what to do when a cell is refined: where should the new
vertices at the edge midpoints and the cell midpoint be located so that the
child cells better represent the desired geometry than the parent cell.

In the code above, we already do this for faces that sit at the boundary:
we use the code
@code
          static const SphericalManifold<dim> boundary;
          triangulation.set_all_manifold_ids_on_boundary(0);
          triangulation.set_manifold (0, boundary);
@endcode
to tell the Triangulation where to ask when refining a boundary face.
To make the mesh <i>interior</i> also track a circular domain, we need to work
a bit harder, though.

First, recall that our coarse mesh consists of a central square
cell and four cells around it. Now first consider what would happen if we
also attached the SphericalManifold object not only to the four exterior faces
but also the four cells at the perimeter as well as all of their faces. We can
do this by replacing the existing mesh creation code
@code
          GridGenerator::hyper_ball (triangulation);

          static const SphericalManifold<dim> boundary;
          triangulation.set_all_manifold_ids_on_boundary(0);
          triangulation.set_manifold (0, boundary);

          triangulation.refine_global (1);
@endcode
by the following snippet (testing that the center of a cell is larger
than a small multiple, say one tenth, of the cell diameter away from
center of the mesh only fails for the central square of the mesh):
@code
          GridGenerator::hyper_ball (triangulation);

          static const SphericalManifold<dim> boundary;
          triangulation.set_all_manifold_ids_on_boundary(0);
          triangulation.set_manifold (0, boundary);

          const Point<dim> mesh_center;
          for (typename Triangulation<dim>::active_cell_iterator cell = triangulation.begin_active();
               cell != triangulation.end(); ++cell)
            if (mesh_center.distance (cell->center()) > cell->diameter()/10)
              cell->set_all_manifold_ids (0);
	  
          triangulation.refine_global (1);
@endcode

After a few global refinement steps, this would lead to a mesh of the following
kind:

<img src="https://www.dealii.org/images/steps/developer/step-6.manifold-grid-4-bad.png" alt="">

This is not a good mesh: the central cell has been refined in such a way that
the children located in the four corners of the original central cell
<i>degenerate</i>: they all tend towards triangles as mesh refinement
continues. This means that the Jacobian matrix of the transformation from
reference cell to actual cell degenerates for these cells, and because
all error estimates for finite element solutions contain the norm of the
inverse of the Jacobian matrix, you will get very large errors on these
cells and, in the limit as mesh refinement, a loss of convergence order because
the cells in these corners become worse and worse under mesh refinement.

So we need something smarter. To this end, consider the following solution
originally developed by Konstantin Ladutenko. We will use the following code:
@code
          GridGenerator::hyper_ball (triangulation);

          static const SphericalManifold<dim> boundary;
          triangulation.set_all_manifold_ids_on_boundary(0);
          triangulation.set_manifold (0, boundary);

          const Point<dim> mesh_center;
          const double core_radius  = 1.0/5.0,
                       inner_radius = 1.0/3.0;

          // Step 1: Shrink the inner cell
          //
          // We cannot get a circle out of the inner cell because of
          // the degeneration problem mentioned above. Rather, shrink
          // the inner cell to a core radius of 1/5 that stays
          // sufficiently far away from the place where the
          // coefficient will have a discontinuity and where we want
          // to have cell interfaces that actually lie on a circle.
          // We do this shrinking by just scaling the location of each
          // of the vertices, given that the center of the circle is
          // simply the origin of the coordinate system.
          for (typename Triangulation<dim>::active_cell_iterator cell = triangulation.begin_active();
               cell != triangulation.end(); ++cell)
            if (mesh_center.distance (cell->center()) < 1e-5)
              {
                for (unsigned int v=0;
                     v < GeometryInfo<dim>::vertices_per_cell;
                     ++v)
                  cell->vertex(v) *= core_radius/mesh_center.distance (cell->vertex(v));
              }

          // Step 2: Refine all cells except the central one
          for (typename Triangulation<dim>::active_cell_iterator cell = triangulation.begin_active();
               cell != triangulation.end(); ++cell)
            if (mesh_center.distance (cell->center()) >= 1e-5)
              cell->set_refine_flag ();
          triangulation.execute_coarsening_and_refinement ();


          // Step 3: Resize the inner children of the outer cells
          //
          // The previous step replaced each of the four outer cells
          // by its four children, but the radial distance at which we
          // have intersected is not what we want to later refinement
          // steps. Consequently, move the vertices that were just
          // created in radial direction to a place where we need
          // them.
          for (typename Triangulation<dim>::active_cell_iterator cell = triangulation.begin_active();
               cell != triangulation.end(); ++cell)
            for (unsigned int v=0; v < GeometryInfo<dim>::vertices_per_cell; ++v)
              {
                const double dist = mesh_center.distance (cell->vertex(v));
                if (dist > core_radius*1.0001 && dist < 0.9999)
                  cell->vertex(v) *= inner_radius/dist;
              }

          // Step 4: Apply curved manifold description
          //
          // As discussed above, we can not expect to subdivide the
          // inner four cells (or their faces) onto concentric rings,
          // but we can do so for all other cells that are located
          // outside the inner radius. To this end, we loop over all
          // cells and determine whether it is in this zone. If it
          // isn't, then we set the manifold description of the cell
          // and all of its bounding faces to the one that describes
          // the spherical manifold already introduced above and that
          // will be used for all further mesh refinement.
          for (typename Triangulation<dim>::active_cell_iterator cell = triangulation.begin_active();
               cell != triangulation.end(); ++cell)
            {
              bool is_in_inner_circle = false;
              for (unsigned int v=0; v < GeometryInfo<2>::vertices_per_cell; ++v)
                if (mesh_center.distance (cell->vertex(v)) < inner_radius)
                  {
                    is_in_inner_circle = true;
                    break;
                  }

              if (is_in_inner_circle == false)
                cell->set_all_manifold_ids (0);
            }
@endcode

This code then generates the following, much better sequence of meshes:

<TABLE WIDTH="100%">
<tr>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.manifold-grid-0.png" alt="">
</td>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.manifold-grid-1.png" alt="">
</td>
</tr>

<tr>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.manifold-grid-2.png" alt="">
</td>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.manifold-grid-3.png" alt="">
</td>
</tr>

<tr>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.manifold-grid-4.png" alt="">
</td>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.manifold-grid-5.png" alt="">
</td>
</tr>

<tr>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.manifold-grid-6.png" alt="">
</td>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-6.manifold-grid-7.png" alt="">
</td>
</tr>
</table>
 
Creating good meshes, and in particular making them fit the geometry you
want, is a complex topic in itself. You can find much more on this in
step-49, step-53, and step-54, among other tutorial programs that cover
the issue. Information on curved domains can also be found in the
documentation module on @ref manifold "Manifold descriptions".

Why does it make sense to choose a mesh that tracks the internal
interface? There are a number of reasons, but the most essential one
comes down to what we actually integrate in our bilinear
form. Conceptually, we want to integrate the term $A_{ij}^K=\int_K
a(\mathbf x) \nabla \varphi_i(\mathbf x) \nabla \varphi_j(\mathbf x) ; dx$ as the
contribution of cell $K$ to the matrix entry $A_{ij}$. We can not
compute it exactly and have to resort to quadrature. We know that
quadrature is accurate if the integrand is smooth. That is because
quadrature in essence computes a polynomial approximation to the
integrand that coincides with the integrand in the quadrature points,
and then computes the volume under this polynomial as an approximation
to the volume under the original integrand. This polynomial
interpolant is accurate if the integrand is smooth on a cell, but it
is usually rather inaccurate if the integrand is discontinuous on a
cell.

Consequently, it is worthwhile to align cells in such a way that the
interfaces across which the coefficient is discontinuous are aligned
with cell interfaces. This way, the coefficient is constant on each
cell, following which the integrand will be smooth, and its polynomial
approximation and the quadrature approximation of the integral will
both be accurate. Note that such an alignment is common in many
practical cases, so deal.II provides a number of functions (such as
@ref GlossMaterialId "material_id") to help manage such a scenario.
Refer to step-28 and step-46 for examples of how material id's can be
applied.

Finally, let us consider the case of a coefficient that has a smooth
and non-uniform distribution in space. We can repeat once again all of
the above discussion on the representation of such a function with the
quadrature. So, to simulate it accurately there are a few readily
available options: you could reduce the cell size, increase the order
of the polynomial used in the quadrature formula, select a more
appropriate quadrature formula, or perform a combination of these
steps. The key is that providing the best fit of the coefficient's
spatial dependence with the quadrature polynomial will lead to a more
accurate finite element solution of the PDE.

<h4>Playing with the regularity of the solution</h4>

From a mathematical perspective, solutions of the Laplace equation
@f[
  -\Delta u = f
@f]
on smoothly bounded, convex domains are known to be smooth themselves. The exact degree
of smoothness, i.e., the function space in which the solution lives, depends
on how smooth exactly the boundary of the domain is, and how smooth the right
hand side is. Some regularity of the solution may be lost at the boundary, but
one generally has that the solution is twice more differentiable in
compact subsets of the domain than the right hand side.
If, in particular, the right hand side satisfies $f\in C^\infty(\Omega)$, then   
$u \in C^\infty(\Omega_i)$ where $\Omega_i$ is any compact subset of $\Omega$
($\Omega$ is an open domain, so a compact subset needs to keep a positive distance
from $\partial\Omega$).

The situation we chose for the current example is different, however: we look
at an equation with a non-constant coefficient $a(\mathbf x)$:
@f[
  -\nabla \cdot (a \nabla u) = f.
@f]
Here, if $a$ is not smooth, then the solution will not be smooth either, 
regardless of $f$. In particular, we expect that wherever $a$ is discontinuous
along a line (or along a plane in 3d),
the solution will have a kink. This is easy to see: if for example $f$
is continuous, then $f=-\nabla \cdot (a \nabla u)$ needs to be 
continuous. This means that $a \nabla u$ must be continuously differentiable 
(not have a kink). Consequently, if $a$ has a discontinuity, then $\nabla u$
must have an opposite discontinuity so that the two exactly cancel and their
product yields a function without a discontinuity. But for $\nabla u$ to have
a discontinuity, $u$ must have a kink. This is of course exactly what is
happening in the current example, and easy to observe in the pictures of the
solution.

In general, if the coefficient $a(\mathbf x)$ is discontinuous along a line in 2d,
or a plane in 3d, then the solution may have a kink, but the gradient of the
solution will not go to infinity. That means, that the solution is at least
still in the space $W^{1,\infty}$. On the other hand, we know that in the most
extreme cases -- i.e., where the domain has reentrant corners, the
right hand side only satisfies $f\in H^{-1}$, or the coefficient $a$ is only in
$L^\infty$ -- all we can expect is that $u\in H^1$, a much larger space than
$W^{1,\infty}$. It is not very difficult to create cases where
the solution is in a space $H^{1+s}$ where we can get $s$ to become as small
as we want. Such cases are often used to test adaptive finite element
methods because the mesh will have to resolve the singularity that causes
the solution to not be in $W^{1,\infty}$ any more. 

The typical example one uses for this is called the <i>Kellogg problem</i>
(referring to the paper "On the Poisson equation with intersecting interfaces"
by R. B. Kellogg, Applicable Analysis, vol. 4, pp. 101-129, 1974), which
in the commonly used form has a coefficient $a(\mathbf x)$ that has different values
in the four quadrants of the plane (or eight different values in the octants
of ${\mathbb R}^3$). The exact degree of regularity (the $s$ in the
index of the Sobolev space above) depends on the values of $a(\mathbf x)$ coming
together at the origin, and by choosing the jumps large enough, the
regularity of the solution can be made as close as desired to $H^1$.

To implement something like this, one could replace the coefficient
function by the following (shown here only for the 2d case):
@code
template <int dim>
double coefficient (const Point<dim> &p)
{
  if ((p[0] < 0) && (p[1] < 0))           // lower left quadrant
    return 1;
  else if ((p[0] >= 0) && (p[1] < 0))     // lower right quadrant
    return 10;
  else if ((p[0] < 0) && (p[1] >= 0))     // upper left quadrant
    return 100;
  else if ((p[0] >= 0) && (p[1] >= 0))    // upper right quadrant
    return 1000;
  else
    {
      Assert (false, ExcInternalError());
      return 0;
    }
}
@endcode
(Adding the <code>Assert</code> at the end ensures that an exception
is thrown if we ever get to that point -- which of course we shouldn't,
but this is a good way to insure yourself: we all make mistakes by
sometimes not thinking of all cases, for example by checking
for <code>p[0]</code> to be less than and greater than zero,
rather than greater-or-equal to zero, and thereby forgetting
some cases that would otherwise lead to bugs that are awkward
to find. The <code>return 0;</code> at the end is only there to
avoid compiler warnings that the function does not end in a
<code>return</code> statement -- the compiler cannot see that the
function would never actually get to that point because of the
preceding <code>Assert</code> statement.)

By playing with such cases where four or more sectors come
together and on which the coefficient has different values, one can
construct cases where the solution has singularities at the
origin. One can also see how the meshes are refined in such cases.
