<h1>Results</h1>

When you run the program, the screen output should look like the following:
@code
Cycle 0:
   Number of active cells:       56 (2 global levels)
   Workload imbalance:           1.14286
   Number of degrees of freedom: 665 (by level: 117, 665)
   Number of CG iterations:      10
   Wrote solution_00.pvtu


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |    0.0457s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble right hand side        |         1 |  0.000241s |      0.53% |
| Estimate                        |         1 |    0.0288s |        63% |
| Output results                  |         1 |   0.00219s |       4.8% |
| Setup                           |         1 |   0.00264s |       5.8% |
| Setup multigrid                 |         1 |   0.00261s |       5.7% |
| Solve                           |         1 |   0.00355s |       7.8% |
| Solve: 1 multigrid V-cycle      |         1 |  0.000315s |      0.69% |
| Solve: CG                       |         1 |   0.00186s |       4.1% |
| Solve: Preconditioner setup     |         1 |  0.000968s |       2.1% |
+---------------------------------+-----------+------------+------------+

Cycle 1:
   Number of active cells:       126 (3 global levels)
   Workload imbalance:           1.17483
   Number of degrees of freedom: 1672 (by level: 117, 665, 1100)
   Number of CG iterations:      11
   Wrote solution_01.pvtu


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |    0.0433s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble right hand side        |         1 |  0.000286s |      0.66% |
| Estimate                        |         1 |    0.0272s |        63% |
| Output results                  |         1 |   0.00333s |       7.7% |
| Refine grid                     |         1 |   0.00196s |       4.5% |
| Setup                           |         1 |    0.0023s |       5.3% |
| Setup multigrid                 |         1 |   0.00262s |         6% |
| Solve                           |         1 |   0.00549s |        13% |
| Solve: 1 multigrid V-cycle      |         1 |  0.000343s |      0.79% |
| Solve: CG                       |         1 |   0.00293s |       6.8% |
| Solve: Preconditioner setup     |         1 |   0.00174s |         4% |
+---------------------------------+-----------+------------+------------+

Cycle 2:
.
.
.
@endcode
Here, the timing of the `solve()` function is split up in 3 parts: setting
up the multigrid preconditioner, execution of a single multigrid V-cycle, and
the CG solver. The V-cycle that is timed is unnecessary for the overall solve
and only meant to give an insight at the different costs for AMG and GMG.
Also it should be noted that when using the AMG solver, "Workload imbalance"
is not included in the output since the hierarchy of coarse meshes is not
required.

All results in this section are gathered on Intel Xeon Platinum 8280 (Cascade
Lake) nodes which have 56 cores and 192GB per node and support AVX-512 instructions,
allowing for vectorization over 8 doubles (vectorization used only in the matrix-free
computations). The code is compiled using gcc 7.1.0 with intel-mpi 17.0.3. Trilinos
12.10.1 is used for the matrix-based GMG/AMG computations.

The following table gives weak scale timings for this program on up to 256M DoFs
and 7168 processors. Here, $\mathbb{E}$ is the partition efficiency from the
 introduction (also equal to 1.0/workload imbalance), "Setup" is a combination
of setup, setup multigrid, assemble, and assemble multigrid from the timing blocks,
and "Prec" is the preconditioner setup. Ideally all times would stay constant
over each problem size for the individual solvers, but since the partition
efficiency decreases from 0.371 to 0.161 from largest to smallest problem size,
we expect to see an approximately $0.371/0.161=2.3$ times increase in timings
for GMG.

<table align="center" class="doxtable">
<tr>
  <th colspan="4"></th>
  <th></th>
  <th colspan="4">MF-GMG</th>
  <th></th>
  <th colspan="4">MB-GMG</th>
  <th></th>
  <th colspan="4">AMG</th>
</tr>
<tr>
  <th align="right">Procs</th>
  <th align="right">Cycle</th>
  <th align="right">DoFs</th>
  <th align="right">$\mathbb{E}$</th>
  <th></th>
  <th align="right">Setup</th>
  <th align="right">Prec</th>
  <th align="right">Solve</th>
  <th align="right">Total</th>
  <th></th>
  <th align="right">Setup</th>
  <th align="right">Prec</th>
  <th align="right">Solve</th>
  <th align="right">Total</th>
  <th></th>
  <th align="right">Setup</th>
  <th align="right">Prec</th>
  <th align="right">Solve</th>
  <th align="right">Total</th>
</tr>
<tr>
  <td align="right">112</th>
  <td align="right">13</th>
  <td align="right">4M</th>
  <td align="right">0.37</th>
  <td></th>
  <td align="right">0.742</th>
  <td align="right">0.393</th>
  <td align="right">0.200</th>
  <td align="right">1.335</th>
  <td></th>
  <td align="right">1.714</th>
  <td align="right">2.934</th>
  <td align="right">0.716</th>
  <td align="right">5.364</th>
  <td></th>
  <td align="right">1.544</th>
  <td align="right">0.456</th>
  <td align="right">1.150</th>
  <td align="right">3.150</th>
</tr>
<tr>
  <td align="right">448</th>
  <td align="right">15</th>
  <td align="right">16M</th>
  <td align="right">0.29</th>
  <td></th>
  <td align="right">0.884</th>
  <td align="right">0.535</th>
  <td align="right">0.253</th>
  <td align="right">1.672</th>
  <td></th>
  <td align="right">1.927</th>
  <td align="right">3.776</th>
  <td align="right">1.190</th>
  <td align="right">6.893</th>
  <td></th>
  <td align="right">1.544</th>
  <td align="right">0.456</th>
  <td align="right">1.150</th>
  <td align="right">3.150</th>
</tr>
<tr>
  <td align="right">1792</th>
  <td align="right">17</th>
  <td align="right">65M</th>
  <td align="right">0.22</th>
  <td></th>
  <td align="right">1.122</th>
  <td align="right">0.686</th>
  <td align="right">0.309</th>
  <td align="right">2.117</th>
  <td></th>
  <td align="right">2.171</th>
  <td align="right">4.862</th>
  <td align="right">1.660</th>
  <td align="right">8.693</th>
  <td></th>
  <td align="right">1.654</th>
  <td align="right">0.546</th>
  <td align="right">1.460</th>
  <td align="right">3.660</th>
</tr>
<tr>
  <td align="right">7168</th>
  <td align="right">19</th>
  <td align="right">256M</th>
  <td align="right">0.16</th>
  <td></th>
  <td align="right">1.214</th>
  <td align="right">0.893</th>
  <td align="right">0.521</th>
  <td align="right">2.628</th>
  <td></th>
  <td align="right">2.386</th>
  <td align="right">7.260</th>
  <td align="right">2.560</th>
  <td align="right">12.206</th>
  <td></th>
  <td align="right">1.844</th>
  <td align="right">1.010</th>
  <td align="right">1.890</th>
  <td align="right">4.744</th>
</tr>
</table>

The following figure gives the strong scaling for each method for cycle 16
(32M DoFs) and 19 (256M DoFs) on between 56 to 28672 processors. While the
matrix-based GMG solver and AMG scale similarly and have a similar time to
solution, the matrix-free GMG solver scales much better and solves the finer
problem in roughly the same time as the AMG solver for the coarser mesh with
only an eighth of the number of unknowns.

<img width="400px" src="https://www.dealii.org/images/steps/developer/step-50-strong-scaling.png" alt="">


<h3> Possible extensions </h3>

<h4> Testing convergence and higher order elements </h4>

The finite element degree is currently hard-coded as 2, see the template
arguments of the main class. It is easy to change. To test, it would be
interesting to switch to a test problem with a reference solution. This way,
you can compare error rates.

<h4> Coarse solver </h4>

A more interesting example would involve a complicated coarse mesh (see
step-49 for inspiration). This then requires a more sophisticated coarse
solver for the geometric multigrid methods. A common approach here is a switch
to AMG by assembling the coarse matrix (even for the matrix-free version).
