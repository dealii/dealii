<h1>Results</h1>


The program generates two kinds of output. The first are the output
files <code>solution-adaptive-q1.vtk</code>,
<code>solution-global-q1.vtk</code>, and
<code>solution-global-q2.vtk</code>. We show the latter in a 3d view
here:


<img src="https://dealii.org/images/steps/developer/step-7.solution.png" alt="">




Secondly, the program writes tables not only to disk, but also to the
screen while running. The output looks like the following (recall that
columns labeled as "H1" actually show the $H^1$ <i>semi-</i>norm
of the error, not the full $H^1$ norm):


@code
examples/\step-7> make run
Solving with Q1 elements, adaptive refinement
=============================================

Cycle 0:
   Number of active cells:       64
   Number of degrees of freedom: 81
Cycle 1:
   Number of active cells:       121
   Number of degrees of freedom: 154
Cycle 2:
   Number of active cells:       280
   Number of degrees of freedom: 341
Cycle 3:
   Number of active cells:       565
   Number of degrees of freedom: 678
Cycle 4:
   Number of active cells:       1075
   Number of degrees of freedom: 1240
Cycle 5:
   Number of active cells:       2041
   Number of degrees of freedom: 2306
Cycle 6:
   Number of active cells:       3913
   Number of degrees of freedom: 4216
Cycle 7:
   Number of active cells:       7432
   Number of degrees of freedom: 7909
Cycle 8:
   Number of active cells:       14203
   Number of degrees of freedom: 14870

cycle cells dofs     L2        H1      Linfty
    0    64    81 1.840e+00 2.858e+00 1.835e+00
    1   121   154 5.336e-02 1.200e+00 1.354e-01
    2   280   341 1.439e-02 7.892e-01 7.554e-02
    3   565   678 8.696e-03 5.086e-01 2.843e-02
    4  1075  1240 3.245e-03 3.059e-01 1.072e-02
    5  2041  2306 2.407e-03 2.147e-01 5.156e-03
    6  3913  4216 8.501e-04 1.503e-01 2.033e-03
    7  7432  7909 7.113e-04 1.086e-01 1.808e-03
    8 14203 14870 3.140e-04 7.671e-02 7.181e-04

Solving with Q1 elements, global refinement
===========================================

Cycle 0:
   Number of active cells:       64
   Number of degrees of freedom: 81
Cycle 1:
   Number of active cells:       256
   Number of degrees of freedom: 289
Cycle 2:
   Number of active cells:       1024
   Number of degrees of freedom: 1089
Cycle 3:
   Number of active cells:       4096
   Number of degrees of freedom: 4225
Cycle 4:
   Number of active cells:       16384
   Number of degrees of freedom: 16641

cycle cells dofs     L2        H1      Linfty
    0    64    81 1.840e+00 2.858e+00 1.835e+00
    1   256   289 3.570e-02 1.199e+00 1.307e-01
    2  1024  1089 1.192e-02 7.565e-01 7.168e-02
    3  4096  4225 3.047e-03 3.823e-01 2.128e-02
    4 16384 16641 7.660e-04 1.917e-01 5.554e-03

n cells         H1                   L2
0    64 2.858e+00    -    - 1.840e+00     -    -
1   256 1.199e+00 2.38 1.25 3.570e-02 51.54 5.69
2  1024 7.565e-01 1.58 0.66 1.192e-02  2.99 1.58
3  4096 3.823e-01 1.98 0.98 3.047e-03  3.91 1.97
4 16384 1.917e-01 1.99 1.00 7.660e-04  3.98 1.99

Solving with Q2 elements, adaptive refinement
===========================================

Cycle 0:
   Number of active cells:       64
   Number of degrees of freedom: 289
Cycle 1:
   Number of active cells:       121
   Number of degrees of freedom: 569
Cycle 2:
   Number of active cells:       280
   Number of degrees of freedom: 1317
Cycle 3:
   Number of active cells:       529
   Number of degrees of freedom: 2459
Cycle 4:
   Number of active cells:       1015
   Number of degrees of freedom: 4719
Cycle 5:
   Number of active cells:       1963
   Number of degrees of freedom: 9039
Cycle 6:
   Number of active cells:       3727
   Number of degrees of freedom: 17143
Cycle 7:
   Number of active cells:       7081
   Number of degrees of freedom: 32343
Cycle 8:
   Number of active cells:       13525
   Number of degrees of freedom: 60895

cycle cells dofs     L2        H1      Linfty
    0    64   289 1.606e-01 1.278e+00 3.029e-01
    1   121   569 7.916e-03 5.257e-01 4.857e-02
    2   280  1317 1.092e-03 1.165e-01 4.832e-03
    3   529  2459 5.999e-04 5.177e-02 1.873e-03
    4  1015  4719 2.100e-04 3.245e-02 7.938e-04
    5  1963  9039 7.821e-05 1.990e-02 7.261e-04
    6  3727 17143 2.868e-05 8.498e-03 1.462e-04
    7  7081 32343 1.146e-05 4.360e-03 8.576e-05
    8 13525 60895 3.747e-06 2.123e-03 2.174e-05

Solving with Q2 elements, global refinement
===========================================

Cycle 0:
   Number of active cells:       64
   Number of degrees of freedom: 289
Cycle 1:
   Number of active cells:       256
   Number of degrees of freedom: 1089
Cycle 2:
   Number of active cells:       1024
   Number of degrees of freedom: 4225
Cycle 3:
   Number of active cells:       4096
   Number of degrees of freedom: 16641
Cycle 4:
   Number of active cells:       16384
   Number of degrees of freedom: 66049

cycle cells dofs     L2        H1      Linfty
    0    64   289 1.606e-01 1.278e+00 3.029e-01
    1   256  1089 7.638e-03 5.248e-01 4.816e-02
    2  1024  4225 8.601e-04 1.086e-01 4.827e-03
    3  4096 16641 1.107e-04 2.756e-02 7.804e-04
    4 16384 66049 1.394e-05 6.915e-03 9.991e-05

n cells         H1                   L2
0    64 1.278e+00    -    - 1.606e-01     -    -
1   256 5.248e-01 2.43 1.28 7.638e-03 21.03 4.39
2  1024 1.086e-01 4.83 2.27 8.601e-04  8.88 3.15
3  4096 2.756e-02 3.94 1.98 1.107e-04  7.77 2.96
4 16384 6.915e-03 3.99 1.99 1.394e-05  7.94 2.99
@endcode


One can see the error reduction upon grid refinement, and for the
cases where global refinement was performed, also the convergence
rates can be seen. The linear and quadratic convergence rates of Q1
and Q2 elements in the $H^1$ semi-norm can clearly be seen, as
are the quadratic and cubic rates in the $L_2$ norm.




Finally, the program also generated LaTeX versions of the tables (not shown
here) that is written into a file in a way so that it could be
copy-pasted into a LaTeX document.


<h3> When is the error "small"? </h3>

What we showed above is how to determine the size of the error
$\|u-u_h\|$ in a number of different norms. We did this primarily
because we were interested in testing that our solutions *converge*.
But from an engineering perspective, the question is often more
practical: How fine do I have to make my mesh so that the error is
"small enough"? In other words, if in the table above the $H^1$
semi-norm has been reduced to `2.123e-03`, is this good enough for me
to sign the blueprint and declare that our numerical simulation showed
that the bridge is strong enough?

In practice, we are rarely in this situation because I can not
typically compare the numerical solution $u_h$ against the exact
solution $u$ in situations that matter -- if I knew $u$, I would not
have to compute $u_h$. But even if I could, the question to ask in
general is then: `2.123e-03` *what*? The solution will have physical
units, say kg-times-meter-squared, and I'm integrating a function with
units square of the above over the domain, and then take the square
root. So if the domain is two-dimensional, the units of
$\|u-u_h\|_{L_2}$ are kg-times-meter-cubed. The question is then: Is
$2.123\times 10^{-3}$ kg-times-meter-cubed small? That depends on what
you're trying to simulate: If you're an astronomer used to masses
measured in solar masses and distances in light years, then yes, this
is a fantastically small number. But if you're doing atomic physics,
then no: That's not small, and your error is most certainly not
sufficiently small; you need a finer mesh.

In other words, when we look at these sorts of numbers, we generally
need to compare against a "scale", i.e., a typical number of the units in
question. One way to do that is to not look
at the *absolute* error $\|u-u_h\|$ in whatever norm, but at the *relative* error
$\|u-u_h\|/\|u\|$. If this ratio is $10^{-5}$, then
you know that *on average*, the difference between $u$ and $u_h$ is
0.001 per cent -- probably small enough for engineering purposes.

How do we compute $\|u\|$? We just need to do an integration loop over
all cells, quadrature points on these cells, and then sum things up
and take the square root at the end. But there is a simpler way often
used: You can call
@code
    Vector<double> zero_vector (dof_handler.n_dofs());
    Vector<float> norm_per_cell(triangulation.n_active_cells());
    VectorTools::integrate_difference(dof_handler,
                                      zero_vector,
                                      Solution<dim>(),
                                      norm_per_cell,
                                      QGauss<dim>(fe->degree + 1),
                                      VectorTools::L2_norm);
@endcode
which computes $\|u-0\|_{L_2}$. Alternatively, if you're particularly
lazy and don't feel like creating the `zero_vector`, you could use
that if the mesh is not too coarse, then $\|u\| \approx \|u_h\|$, and
we can compute $\|u\| \approx \|u_h\|=\|0-u_h\|$ by calling
@code
    Vector<float> norm_per_cell(triangulation.n_active_cells());
    VectorTools::integrate_difference(dof_handler,
                                      solution,
                                      Functions::ZeroFunction<dim>(),
                                      norm_per_cell,
                                      QGauss<dim>(fe->degree + 1),
                                      VectorTools::L2_norm);
@endcode
In both cases, one then only has to combine the vector of cellwise
norms into one global norm as we already do in the program, by calling
@code
    const double L2_norm =
      VectorTools::compute_global_error(triangulation,
                                        norm_per_cell,
                                        VectorTools::L2_norm);
@endcode

What you will find if you inserted this kind of code into the program
is that $\|u\|_{L_2(\Omega)} \approx 0.27135$. So in the computations
above, if you have $\|u-u_h\|_{L_2(\Omega)} = 3.140e-04$ (the last
value for adaptive refinement using $Q_1$ elements, then this means
that the *relative* error is
@f{align*}{
  \frac{\|u-u_h\|_{L_2(\Omega)}}{\|u\|_{L_2(\Omega)}}
  \approx 0.00116,
@f}
or an error of approximately around 0.1 per cent -- that's good enough
for most applications! It only took some 14,000 cells to achieve this,
and a few seconds. Of course, the equation here is also particularly
easy to solve, and you will find that for many other problems, and in
particular in 3d, one needs *far* more effort to solve problems to
this level of accuracy.


<h3> Cost-accuracy diagrams for this program </h3>

As mentioned in the introduction, we assess numerical algorithms by
figures that show cost vs. accuracy. The information for this is
mostly available from the data tables produced by this program and
shown above already. Specifically, the program above outputs four
tables (for uniform and adaptive mesh refinement, and $Q_1$ and $Q_2$
finite elements) that already have data points for cost (e.g., the
number of degrees of freedom $N_\text{dofs}$) and accuracy (the error
in one of several different norms). From the number of cells
$N_\text{cells}$ in the table, we can also compute the mesh size at
least in the case of uniformly refined meshes via
$h=\frac{2}{\sqrt{N_\text{cells}}}$ given that we are in 2d and that
the domain has width and height equal to 2. What we don't have is a
measure of run time from the program. This turns out not to be too
difficult to add using the Timer class you will first see in step-28
and that I won't show here so as not too complicate the situation more
than necessary. In any case, adding this information to the tables
above, and running a few more refinement cycles, we can then convert
the resulting data into this little Python program to create figures:

@code{.py}
import numpy as np
import matplotlib.pyplot as plt
import math

...# Q1, adaptive refinement
...#cycle  cells   dofs         L2         H1     Linfty  time (exclusive)  time (inclusive)
data_q1_adaptive = [
    [  0,     64,     81, 1.840e+00, 2.858e+00, 1.835e+00,           0.0010,           0.0010],
    [  1,    121,    154, 5.336e-02, 1.200e+00, 1.354e-01,           0.0009,           0.0020],
    [  2,    280,    341, 1.439e-02, 7.892e-01, 7.554e-02,           0.0031,           0.0050],
    [  3,    565,    678, 8.696e-03, 5.086e-01, 2.843e-02,           0.0062,           0.0112],
    [  4,   1075,   1240, 3.245e-03, 3.059e-01, 1.072e-02,           0.0116,           0.0228],
    [  5,   2041,   2306, 2.407e-03, 2.147e-01, 5.156e-03,           0.0221,           0.0449],
    [  6,   3913,   4216, 8.501e-04, 1.503e-01, 2.033e-03,           0.0174,           0.0623],
    [  7,   7432,   7909, 7.113e-04, 1.086e-01, 1.808e-03,           0.0315,           0.0938],
    [  8,  14203,  14870, 3.140e-04, 7.671e-02, 7.181e-04,           0.0708,           0.1645],
    [  9,  26983,  27974, 2.442e-04, 5.605e-02, 4.535e-04,           0.3129,           0.4775],
    [ 10,  51373,  52648, 7.752e-05, 3.973e-02, 1.753e-04,           0.5939,           1.0714],
    [ 11,  97606,  99481, 6.101e-05, 2.926e-02, 1.051e-04,           1.2738,           2.3452],
    [ 12, 184801, 187576, 3.219e-05, 2.098e-02, 8.162e-05,           2.8018,           5.1470],
    [ 13, 350581, 355204, 1.964e-05, 1.543e-02, 3.971e-05,           6.8079,          11.9549]]

...# Q1, global refinement
...#cycle  cells   dofs         L2         H1     Linfty  time (exclusive)  time (inclusive)
data_q1_global = [
    [  0,     64,     81, 1.840e+00, 2.858e+00, 1.835e+00,           0.0010,           0.0010],
    [  1,    256,    289, 3.570e-02, 1.199e+00, 1.307e-01,           0.0027,           0.0037],
    [  2,   1024,   1089, 1.192e-02, 7.565e-01, 7.168e-02,           0.0049,           0.0086],
    [  3,   4096,   4225, 3.047e-03, 3.823e-01, 2.128e-02,           0.0286,           0.0372],
    [  4,  16384,  16641, 7.660e-04, 1.917e-01, 5.554e-03,           0.1024,           0.1396],
    [  5,  65536,  66049, 1.918e-04, 9.591e-02, 1.403e-03,           0.5169,           0.6565],
    [  6, 262144, 263169, 4.796e-05, 4.797e-02, 3.518e-04,           3.2094,           3.8660]]

...# Q2, adaptive refinement
...#cycle  cells   dofs         L2         H1     Linfty  time (exclusive)  time (inclusive)
data_q2_adaptive = [
    [  0,     64,     289, 1.606e-01, 1.278e+00, 3.029e-01,           0.0023,           0.0023],
    [  1,    121,     569, 7.916e-03, 5.257e-01, 4.857e-02,           0.0029,           0.0052],
    [  2,    280,    1317, 1.092e-03, 1.165e-01, 4.832e-03,           0.0062,           0.0115],
    [  3,    529,    2459, 5.999e-04, 5.177e-02, 1.873e-03,           0.0245,           0.0359],
    [  4,   1015,    4719, 2.100e-04, 3.245e-02, 7.938e-04,           0.0556,           0.0915],
    [  5,   1963,    9039, 7.821e-05, 1.990e-02, 7.261e-04,           0.0606,           0.1521],
    [  6,   3727,   17143, 2.868e-05, 8.498e-03, 1.462e-04,           0.3220,           0.4741],
    [  7,   7081,   32343, 1.146e-05, 4.360e-03, 8.576e-05,           0.6608,           1.1349],
    [  8,  13525,   60895, 3.747e-06, 2.123e-03, 2.174e-05,           1.2863,           2.4212],
    [  9,  25675,  119149, 1.770e-06, 1.419e-03, 1.685e-05,           3.1596,           5.5808],
    [ 10,  48649,  223813, 7.563e-07, 7.145e-04, 8.908e-06,           7.6294,          13.2102],
    [ 11,  92152,  420625, 3.207e-07, 3.726e-04, 2.413e-06,          17.2886,          30.4988],
    [ 12, 174361,  801241, 1.394e-07, 2.344e-04, 1.503e-06,          42.8784,          73.3773],
    [ 13, 329440, 1525936, 6.378e-08, 1.203e-04, 7.240e-07,         103.9731,         177.3504]]

...# Q2, global refinement
...#cycle  cells   dofs         L2         H1     Linfty  time (exclusive)  time (inclusive)
data_q2_global = [
    [  0,     64,     289, 1.606e-01, 1.278e+00, 3.029e-01,           0.0018,           0.0018],
    [  1,    256,    1089, 7.638e-03, 5.248e-01, 4.816e-02,           0.0044,           0.0061],
    [  2,   1024,    4225, 8.601e-04, 1.086e-01, 4.827e-03,           0.0277,           0.0339],
    [  3,   4096,   16641, 1.107e-04, 2.756e-02, 7.804e-04,           0.1383,           0.1721],
    [  4,  16384,   66049, 1.394e-05, 6.915e-03, 9.991e-05,           0.9730,           1.1451],
    [  5,  65536,  263169, 1.761e-06, 1.730e-03, 1.245e-05,           6.6831,           7.8282],
    [  6, 262144, 1050625, 2.563e-07, 4.327e-04, 1.636e-06,          44.0744,          51.9026]]


...# Plot error as a function of the number of DoFs
plt.title("Error as a function of the number of unknowns")
plt.plot([d[2] for d in data_q1_adaptive],            # third column (counted from zero)
         [d[3]/0.27135 for d in data_q1_adaptive],    # fourth column, divided by ||u||
         "-*", label="Q1 element, adaptive mesh refinement")
plt.plot([d[2] for d in data_q1_global],              # third column (counted from zero)
         [d[3]/0.27135 for d in data_q1_global],      # fourth column, divided by ||u||
         "-*", label="Q1 element, uniform mesh refinement")
plt.plot([d[2] for d in data_q2_adaptive],            # third column (counted from zero)
         [d[3]/0.27135 for d in data_q2_adaptive],    # fourth column, divided by ||u||
         "-*", label="Q2 element, adaptive mesh refinement")
plt.plot([d[2] for d in data_q2_global],              # third column (counted from zero)
         [d[3]/0.27135 for d in data_q2_global],      # fourth column, divided by ||u||
         "-*", label="Q2 element, uniform mesh refinement")

plt.xlabel("Number of degrees of freedom")
plt.ylabel("Relative L2 error ||u-uh||/||u||")
plt.xscale("log")
plt.yscale("log")
plt.legend()

plt.show()


...# Plot error as a function of the mesh size
plt.title("Error as a function of the mesh size")
plt.plot([1/math.sqrt(d[1]) for d in data_q1_global],  # 1/sqrt(second column)
         [d[3]/0.27135 for d in data_q1_global],
         "-*", label="Q1 element, uniform mesh refinement")
plt.plot([1/math.sqrt(d[1]) for d in data_q2_global],
         [d[3]/0.27135 for d in data_q2_global],
         "-*", label="Q2 element, uniform mesh refinement")

plt.xlabel("Mesh size h")
plt.ylabel("Relative L2 error ||u-uh||/||u||")
plt.xscale("log")
plt.yscale("log")
plt.legend()

plt.show()


# Plot error as a function of the run time
plt.title("Error as a function of run time")
plt.plot([d[7] for d in data_q1_adaptive],            # inclusive time column
         [d[3]/0.27135 for d in data_q1_adaptive],    # fourth column (counted from zero)
         "-*", label="Q1 element, adaptive mesh refinement")
plt.plot([d[6] for d in data_q1_global],              # exclusive time column
         [d[3]/0.27135 for d in data_q1_global],      # fourth column (counted from zero)
         "-*", label="Q1 element, uniform mesh refinement")
plt.plot([d[7] for d in data_q2_adaptive],            # inclusive time column
         [d[3]/0.27135 for d in data_q2_adaptive],    # fourth column (counted from zero)
         "-*", label="Q2 element, adaptive mesh refinement")
plt.plot([d[6] for d in data_q2_global],              # exclusive time column
         [d[3]/0.27135 for d in data_q2_global],      # fourth column (counted from zero)
         "-*", label="Q2 element, uniform mesh refinement")

plt.xlabel("Run time")
plt.ylabel("Relative L2 error ||u-uh||/||u||")
plt.xscale("log")
plt.yscale("log")
plt.legend()

plt.show()
@endcode

@note When showing run time in the figures below, it is useful to be
clear about what that actually encompasses to fairly represent how
long it takes to compute a data point. The way I've measured this is
as follows: For uniform meshes, we could have just globally refined
the mesh to a specific refinement level and then solve on it; as a
consequence, the run time shown really only shows the time to set up,
assemble, and solve the system *on the mesh for which the time is
reported*. (The table captions in the Python program call this "time
(exclusive)".) On the other hand, for adaptive meshes, one needs to
first go through the coarser meshes, solve on them, refine the mesh,
until one arrives at a specific adaptive mesh; thus, the run times
shown for a specific mesh refinement level correspond to the time to
solve on this mesh *and all previous meshes* (labeled "time
(inclusive)"). Second, when we evaluate the run time of methods, we
should think of these as "production runs" done once a program has
been completely debugged. That is, we should run the program in
"release mode", not the default "debug mode" -- see step-8 for more
details. Finally, when discussing run times, it is useful to state
what kind of machine these results were obtained on; here, it is a 2024
laptop running a 13th generation Intel Core i9-13900H processor.

With this program, we get the three figures, all representing
cost-accuracy diagrams. First, a plot of the error as a function of
the number of unknowns:

<img src="https://dealii.org/images/steps/developer/step-7.cost-accuracy-dofs.png" alt="">

The figure shows that (when measuring the cost in terms of
$N_\text{dofs}$!) $Q_2$ elements are substantially better than $Q_1$
elements because for the same number of unknowns, the error is much
smaller for $Q_2$ elements than for $Q_1$ elements. For example, for the
last $Q_1$ data point, the error for $Q_2$ is smaller by about a
factor of 100x at that number of unknowns. Similarly, one can say that
for a relative error of $10^{-3}$, one needs perhaps a factor of 20x fewer
unknowns with $Q_2$ than with $Q_1$ elements. Similarly, adaptively
refined meshes are always better than uniformly refined meshes.

The second figure created by the program above shows the error as a
function of mesh size. This is only useful for uniformly refined
meshes because the mesh size is of course not the same for all cells
of the adaptively refined meshes:

<img src="https://dealii.org/images/steps/developer/step-7.cost-accuracy-h.png" alt="">

Here, again, we see the superiority of the $Q_2$ element. The two
lines represent the theoretical estimates that for $Q_1$, the $L_2$
error should behave as $\|u-u_h\|_{L_2(\Omega)} \le C h^2$, and for
the $Q_2$ element $\|u-u_h\|_{L_2(\Omega)} \le C h^3$. Indeed, we see can
that a reduction of $h$ by a factor of 10 in the figure corresponds to
reductions of the error by factors of around 100 and 1000,
respectively, as predicted by theory.

The final figure shows the error as a function of run time:

<img src="https://dealii.org/images/steps/developer/step-7.cost-accuracy-run-time.png" alt="">

This figure is more difficult to interpret. First, again, $Q_2$ is
better than $Q_1$: For the same run time, we get substantially lower
errors; or, equivalently, we can reach the same error in substantially
less compute time. The surprise is perhaps that the adaptively refined
meshes are not better than the uniform meshes as far as run time is
concerned. (Though they are better in terms of memory consumption,
because memory is proportional to the number of degrees of freedom
shown above.) This is because (i) to compute on an adaptively refined
mesh, you have to first solve on all previous meshes, and as explained
above this time is included in the figure for the adaptively refined
meshes; (ii) from a theoretical perspective, one can show that
adaptive meshes are only *qualitatively* better than uniform meshes
for solutions that are not smooth, whereas the solution we have chosen
for the program here is $C^\infty$. The solutions of partial
differential equations for *real* situations are not smooth, however,
and so what you will see in other tutorial programs is that adaptive
mesh refinement really *is* better in terms of run time as well.

The cost-accuracy diagrams shown above allow us to compare the four
methods in a quantitative way. But they also allow us to draw one
other conclusion: The figures all credibly convey through their
straight lines that with sufficient effort (measured in any of the
three ways), we can make the error as small as we want (or need, based
on engineering requirements). In other words, they illustrate that our
method *converges* to the exact solution.



<h3> Possibilities for extensions </h3>

<h4> Higher order elements </h4>

Go ahead and run the program with higher order elements ($Q_3$, $Q_4$, ...). You
will notice that assertions in several parts of the code will trigger (for
example in the generation of the filename for the data output). You might have to address these,
but it should not be very hard to get the program to work!

<h4> Better linear solvers </h4>

If you carefully looked at the run time tables shown in the Python
program above, you will have noticed that the
linear system solver we use in this program is not perfect -- its run
time grows faster than proportional to the linear system size -- and
picking a better linear solver might be appropriate for this kind of
comparison.


@anchor step_7-ResultsSection-bad-sign
<h4> Switching the sign of alpha in the equation </h4>

The equation we are solving here is
@f[
  -\Delta u + \alpha u = f,
@f]
with $\alpha=1$. This is sometimes called "the Helmholtz equation with
the good sign" because the fact that $\alpha>0$ makes the equation
fairly easy to solve: The solution minimizes a quadratic energy
@f[
  E(u) = \int_\Omega \frac 12 |\nabla u|^2 + \frac 12 \alpha u^2 - fu,
@f]
with a unique minimizer,
which translates into the fact that the bilinear form we use is
[coercive](https://en.wikipedia.org/wiki/Coercive_function#Coercive_operators_and_forms);
a different way of putting this is by saying that the operator
$-\Delta + \alpha$ is positive definite and
[elliptic](https://en.wikipedia.org/wiki/Elliptic_operator). Solutions of
such equations are typically nice and smooth, and we have many good methods
of solving them efficiently and to high accuracy.

All of this is, in general, not true if $\alpha<0$. As long as $\alpha$ is not
equal to the negative of one of the eigenvalues of the Laplace operator,
the equation
@f[
  -\Delta u + \alpha u = f,
@f]
still has a solution even for $\alpha<0$, but the matrix you get is in
general not nearly as easy to invert as for $\alpha\ge 0$. Moreover,
unless you pick a specific $f$, you will see that the solution is not
nice and smooth any more: Solutions of the "Helmholtz equation with the
bad sign" (i.e., with $\alpha<0$) are in general oscillatory, and the more
negative $\alpha$ is, the shorter the wave length of these oscillations.

You can easily try all of this out with
the program here by changing the bilinear form in the program. (Of course,
if you want to still want to compute errors, you will also have to make
sure that the desired solution and the right hand side match.) The only
other place you have to change is the solver: The matrix you are now
building is no longer positive definite, and so the SolverCG class
(implementing the Conjugate Gradient method) is no longer appropriate.
Instead, try SolverGMRES -- a method that can deal with matrices that are
not definite.
