<br>

<i>
This program grew out of a student project by Sven Wetterauer at the
University of Heidelberg, Germany. Most of the work for this program
is by him.
</i>
<br>


<a name="Intro"></a>
<h1>Introduction</h1>

<h3>Foreword</h3>

This program deals with an example of a non-linear elliptic partial
differential equation, the minimal
surface equation. You can imagine the solution of this equation to describe
the surface spanned by a soap film that is enclosed by a
closed wire loop. We imagine the wire to not just be a planar loop, but in
fact curved. The surface tension of the soap film will then reduce the surface
to have minimal surface. The solution of the minimal surface equation
describes this shape with the wire's vertical displacement as a boundary
condition. For simplicity, we will here assume that the surface can be written
as a graph $u=u(x,y)$ although it is clear that it is not very hard to
construct cases where the wire is bent in such a way that the surface can only
locally be constructed as a graph but not globally.

Because the equation is non-linear, we can't solve it directly. Rather, we
have to use Newton's method to compute the solution iteratively.

@dealiiVideoLecture{31.5,31.55,31.6}
(@dealiiVideoLectureSeeAlso{31.65,31.7})



<h3>Classical formulation</h3>

In a classical sense, the problem is given in the following form:


  @f{align*}
    -\nabla \cdot \left( \frac{1}{\sqrt{1+|\nabla u|^{2}}}\nabla u \right) &= 0 \qquad
    \qquad &&\textrm{in} ~ \Omega
    \\
    u&=g \qquad\qquad &&\textrm{on} ~ \partial \Omega.
  @f}

$\Omega$ is the domain we get by projecting the wire's positions into $x-y$
space. In this example, we choose $\Omega$ as the unit disk.

As described above, we solve this equation using Newton's method in which we
compute the $n$th approximate solution from the $n$th$-1$ one, and use
a damping parameter $\alpha^n$ to get better global convergence behavior:
  @f{align*}
    F'(u^{n},\delta u^{n})&=- F(u^{n})
    \\
    u^{n+1}&=u^{n}+\alpha^n \delta u^{n}
  @f}
with
  @f[
    F(u) \dealcoloneq -\nabla \cdot \left( \frac{1}{\sqrt{1+|\nabla u|^{2}}}\nabla u \right)
  @f]
and $F'(u,\delta u)$ the derivative of F in direction of $\delta u$:
@f[
  F'(u,\delta u)=\lim \limits_{\epsilon \rightarrow 0}{\frac{F(u+\epsilon \delta u)-
  F(u)}{\epsilon}}.
@f]

Going through the motions to find out what $F'(u,\delta u)$ is, we find that
we have to solve a linear elliptic PDE in every Newton step, with $\delta u^n$
as the solution of:

  @f[
  - \nabla \cdot \left( \frac{1}{(1+|\nabla u^{n}|^{2})^{\frac{1}{2}}}\nabla
  \delta u^{n} \right) +
  \nabla \cdot \left( \frac{\nabla u^{n} \cdot
  \nabla \delta u^{n}}{(1+|\nabla u^{n}|^{2})^{\frac{3}{2}}} \nabla u^{n}
  \right)  =
  -\left( - \nabla \cdot \left( \frac{1}{(1+|\nabla u^{n}|^{2})^{\frac{1}{2}}}
  \nabla u^{n} \right) \right)
  @f]

In order to solve the minimal surface equation, we have to solve this equation
repeatedly, once per Newton step. To solve this, we have to take a look at the
boundary condition of this problem. Assuming that $u^{n}$ already has the
right boundary values, the Newton update $\delta u^{n}$ should have zero
boundary conditions, in order to have the right boundary condition after
adding both.  In the first Newton step, we are starting with the solution
$u^{0}\equiv 0$, the Newton update still has to deliver the right boundary
condition to the solution $u^{1}$.


Summing up, we have to solve the PDE above with the boundary condition $\delta
u^{0}=g$ in the first step and with $\delta u^{n}=0$ in all the following steps.


<h3>Weak formulation of the problem</h3>

Starting with the strong formulation above, we get the weak formulation by multiplying
both sides of the PDE with a test function $\varphi$ and integrating by parts on both sides:
  @f[
  \left( \nabla \varphi , \frac{1}{(1+|\nabla u^{n}|^{2})^{\frac{1}{2}}}\nabla
  \delta u^{n} \right)-\left(\nabla \varphi ,\frac{\nabla u^{n} \cdot \nabla
  \delta u^{n}}{(1+|\nabla u^{n}|^{2})^{\frac{3}{2}}}\nabla u^{n}  \right)
  = -\left(\nabla \varphi , \frac{1}{(1+|\nabla u^{n}|^{2})^{\frac{1}{2}}} \nabla u^{n}
   \right).
  @f]
Here the solution $\delta u^{n}$ is a function in $H^{1}(\Omega)$, subject to
the boundary conditions discussed above.
Reducing this space to a finite dimensional space with basis $\left\{
\varphi_{0},\dots , \varphi_{N-1}\right\}$, we can write the solution:

@f[
  \delta u^{n}=\sum_{j=0}^{N-1} \delta U_{j} \varphi_{j}.
@f]

Using the basis functions as test functions and defining $a_{n} \dealcoloneq \frac{1}
{\sqrt{1+|\nabla u^{n}|^{2}}}$, we can rewrite the weak formulation:

@f[
  \sum_{j=0}^{N-1}\left[ \left( \nabla \varphi_{i} , a_{n} \nabla \varphi_{j} \right) -
  \left(\nabla u^{n}\cdot \nabla \varphi_{i} , a_{n}^{3} \nabla u^{n} \cdot \nabla
  \varphi_{j} \right) \right] \cdot \delta U_{j}=-\left( \nabla \varphi_{i} , a_{n}
  \nabla u^{n}\right) \qquad \forall i=0,\dots ,N-1,
@f]

where the solution $\delta u^{n}$ is given by the coefficients $\delta U^{n}_{j}$.
This linear system of equations can be rewritten as:

@f[
  A^{n}\; \delta U^{n}=b^{n},
@f]

where the entries of the matrix $A^{n}$ are given by:

@f[
  A^{n}_{ij} \dealcoloneq \left( \nabla \varphi_{i} , a_{n} \nabla \varphi_{j} \right) -
  \left(\nabla u^{n}\cdot \nabla \varphi_{i} , a_{n}^{3} \nabla u^{n} \cdot \nabla
  \varphi_{j} \right),
@f]

and the right hand side $b^{n}$ is given by:

@f[
  b^{n}_{i} \dealcoloneq -\left( \nabla \varphi_{i} , a_{n} \nabla u^{n}\right).
@f]


<h3> Questions about the appropriate solver </h3>

The matrix that corresponds to the Newton step above can be reformulated to
show its structure a bit better. Rewriting it slightly, we get that it has the
form
@f[
  A_{ij}
  =
  \left(
    \nabla \varphi_i,
    B
    \nabla \varphi_j
  \right),
@f]
where the matrix $B$ (of size $d \times d$ in $d$ space dimensions) is given
by the following expression:
@f[
  B
  =
  a_n \left\{
   \mathbf I
   -
   a_n^2 [\nabla u_n] \otimes [\nabla u_n]
  \right\}
  =
  a_n \left\{
   \mathbf I
   -
  \frac{\nabla u_n}{\sqrt{1+|\nabla u^{n}|^{2}}} \otimes
  \frac{\nabla u_n}{\sqrt{1+|\nabla u^{n}|^{2}}}
  \right\}.
@f]
From this expression, it is obvious that
$B$ is symmetric, and so $A$ is symmetric as well.
On the other hand, $B$ is also positive definite, which confers the same
property onto $A$. This can be seen by noting that the vector $v_1 =
\frac{\nabla u^n}{|\nabla u^n|}$ is an eigenvector of $B$ with eigenvalue
$\lambda_1=a_n \left(1-\frac{|\nabla u^n|^2}{1+|\nabla u^n|^2}\right) > 0$ while all vectors $v_2\ldots v_d$
that are perpendicular to $v_1$ and each other are eigenvectors with
eigenvalue $a_n$. Since all eigenvalues are positive, $B$ is positive definite
and so is $A$. We can thus use the CG method for solving the Newton steps.
(The fact that the matrix $A$ is symmetric and positive definite should not come
as a surprise. It results from taking the derivative of an operator that
results from taking the derivative of an energy functional: the minimal
surface equation simply minimizes some non-quadratic energy. Consequently,
the Newton matrix, as the matrix of second derivatives of a scalar energy,
must be symmetric since the derivative with regard to the $i$th and $j$th
degree of freedom should clearly commute. Likewise, if the energy functional
is convex, then the matrix of second derivatives must be positive definite,
and the direct calculation above simply reaffirms this.)

It is worth noting, however, that the positive definiteness degenerates for
problems where $\nabla u$ becomes large. In other words, if we simply multiply
all boundary values by 2, then to first order $u$ and $\nabla u$ will also be
multiplied by two, but as a consequence the smallest eigenvalue of $B$ will
become smaller and the matrix will become more ill-conditioned. (More
specifically, for $|\nabla u^n|\rightarrow\infty$ we have that
$\lambda_1 \propto a_n \frac{1}{|\nabla u^n|^2}$ whereas
$\lambda_2\ldots \lambda_d=a_n$; thus, the condition number of $B$,
which is a multiplicative factor in the condition number of $A$ grows
like ${\cal O}(|\nabla u^n|^2)$.) It is simple
to verify with the current program that indeed multiplying the boundary values
used in the current program by larger and larger values results in a problem
that will ultimately no longer be solvable using the simple preconditioned CG
method we use here.


<h3> Choice of step length and globalization </h3>

As stated above, Newton's method works by computing a direction
$\delta u^n$ and then performing the update $u^{n+1} = u^{n}+\alpha^n
\delta u^{n}$ with a step length $0 < \alpha^n \le 1$. It is a common
observation that for strongly nonlinear models, Newton's method does
not converge if we always choose $\alpha^n=1$ unless one starts with
an initial guess $u^0$ that is sufficiently close to the solution $u$
of the nonlinear problem. In practice, we don't always have such an
initial guess, and consequently taking full Newton steps (i.e., using
$\alpha=1$) does frequently not work.

A common strategy therefore is to use a smaller step length for the
first few steps while the iterate $u^n$ is still far away from the
solution $u$ and as we get closer use larger values for $\alpha^n$
until we can finally start to use full steps $\alpha^n=1$ as we are
close enough to the solution. The question is of course how to choose
$\alpha^n$. There are basically two widely used approaches: line
search and trust region methods.

In this program, we simply always choose the step length equal to
0.1. This makes sure that for the testcase at hand we do get
convergence although it is clear that by not eventually reverting to
full step lengths we forego the rapid, quadratic convergence that
makes Newton's method so appealing. Obviously, this is a point one
eventually has to address if the program was made into one that is
meant to solve more realistic problems. We will comment on this issue
some more in the <a href="#Results">results section</a>.


<h3> Summary of the algorithm and testcase </h3>

Overall, the program we have here is not unlike step-6 in many regards. The
layout of the main class is essentially the same. On the other hand, the
driving algorithm in the <code>run()</code> function is different and works as
follows:
<ol>
<li>
  Start with the function $u^{0}\equiv 0$ and modify it in such a way
  that the values of $u^0$ along the boundary equal the correct
  boundary values $g$ (this happens in
  <code>MinimalSurfaceProblem::set_boundary_values</code>). Set
  $n=0$.
</li>

<li>
  Compute the Newton update by solving the system $A^{n}\;\delta
  U^{n}=b^{n}$
  with boundary condition $\delta u^{n}=0$ on $\partial \Omega$.
</li>

<li>
  Compute a step length $\alpha^n$. In this program, we always set
  $\alpha^n=0.1$. To make things easier to extend later on, this
  happens in a function of its own, namely in
  <code>MinimalSurfaceProblem::determine_step_length</code>.
</li>

<li>
  The new approximation of the solution is given by
  $u^{n+1}=u^{n}+\alpha^n \delta u^{n}$.
</li>

<li>
  If $n$ is a multiple of 5 then refine the mesh, transfer the
  solution $u^{n+1}$ to the new mesh and set the values of $u^{n+1}$
  in such a way that along the boundary we have
  $u^{n+1}|_{\partial\Gamma}=g$ (again in
  <code>MinimalSurfaceProblem::set_boundary_values</code>). Note that
  this isn't automatically
  guaranteed even though by construction we had that before mesh
  refinement $u^{n+1}|_{\partial\Gamma}=g$ because mesh refinement
  adds new nodes to the mesh where we have to interpolate the old
  solution to the new nodes upon bringing the solution from the old to
  the new mesh. The values we choose by interpolation may be close to
  the exact boundary conditions but are, in general, nonetheless not
  the correct values.
</li>

<li>
  Set $n\leftarrow n+1$ and go to step 2.
</li>
</ol>

The testcase we solve is chosen as follows: We seek to find the solution of
minimal surface over the unit disk $\Omega=\{\mathbf x: \|\mathbf
x\|<1\}\subset {\mathbb R}^2$ where the surface attains the values
$u(x,y)|{\partial\Omega} = g(x,y) \dealcoloneq \sin(2 \pi (x+y))$ along the
boundary.
