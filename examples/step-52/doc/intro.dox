<br>

<i>This program was contributed by Bruno Turcksin and Damien Lebrun-Grandie.</i>

<b>Note:</b> In order to run this program, deal.II must be configured to use
the UMFPACK sparse direct solver. Refer to the <a
href="../../readme.html#umfpack">ReadMe</a> for instructions how to do this.

<a name="Intro"></a>
<h1>Introducion</h1>

This program shows how to use Runge-Kutta methods to solve a time-dependent
problem. It solves a small variation of the heat equation discussed first in
step-26 but, since the purpose of this program is only to demonstrate using
more advanced ways to interface with deal.II's time stepping algorithms, only
solves a simple problem on a uniformly refined mesh. 


<h3>Problem statement</h3>

In this example, we solve the one-group time-dependent diffusion
approximation of the neutron transport equation (see step-28 for the
time-independent multigroup diffusion). This is a model for how neutrons move
around highly scattering media, and consequently it is a variant of the
time-dependent diffusion equation -- which is just a different name for the
heat equation discussed in step-26, plus some extra terms.
We assume that the medium is not
fissible and therefore, the neutron flux satisfies the following equation:
@f{eqnarray*}
\frac{1}{v}\frac{\partial \phi(x,t)}{\partial t} = \nabla \cdot D(x) \nabla \phi(x,t)
- \Sigma_a(x) \phi(x,t) + S(x,t)
@f}
augmented by appropriate boundary conditions. Here, $v$ is the velocity of
neutrons (for simplicity we assume it is equal to 1 which can be achieved by
simply scaling the time variable), $D$ is the diffusion coefficient, 
$\Sigma_a$ is the absorption cross section, and $S$ is a source. Because we are
only interested in the time dependence, we assume that $D$ and $\Sigma_a$ are
constant. 

Since this program only intends to demonstrate how to use advanced time
stepping algorithms, we will only look for the solutions of relatively simple
problems. Specifically, we are looking for a solution on a square domain
$[0,b]\times[0,b]$ of the form
@f{eqnarray*}
\phi(x,t) = A\sin(\omega t)(bx-x^2).
@f}
By using quadratic finite elements, we can represent this function exactly at
any particular time, and all the error will be due to the time discretization. We
impose the following boundary conditions: homogeneous Dirichlet for $x=0$ and
$x=b$ and homogeneous Neumann conditions for $y=0$ and $y=b$. We choose the
source term so that the corresponding solution is
in fact of the form stated above:
@f{eqnarray*}
S=A\left(\frac{1}{v}\omega \cos(\omega t)(bx -x^2) + \sin(\omega t)
\left(\Sigma_a (bx-x^2)+2D\right) \right).
@f}
Because the solution is a sine, we know that $\phi\left(x,\pi\right) = 0$.
Therefore, the error at time $t=\pi$ is simply the norm of the numerical
solution and is particularly easily evaluated.


<h3>Runge-Kutta methods</h3>

The Runge-Kutta methods implemented in deal.II assume that the equation to be
solved can be written as:
@f{eqnarray*}
\frac{dy}{dt} = g(t,y).
@f}
On the other hand, when using finite elements, discretized time derivatives always result in the
presence of a mass matrix on the left hand side. This can easily be seen by
considering that if the solution vector $y(t)$ in the equation above is in fact the vector
of nodal coefficients $U(t)$ for a variable of the form
@f{eqnarray*}
  u_h(x,t) = \sum_j U_j(t) \varphi_j(x)
@f}
with spatial shape functions $\varphi_j(x)$, then multiplying an equation of
the form 
@f{eqnarray*}
  \frac{\partial u(x,t)}{\partial t} = q(t,u(x,t))
@f}
by test functions, integrating over $\Omega$, substituting $u\rightarrow u_h$
and restricting the test functions to the $\varphi_i(x)$ from above, then this
spatially discretized equation has the form
@f{eqnarray*}
M\frac{dU}{dt} = f(t,U),
@f}
where $M$ is the mass matrix and $f(t,U)$ is the spatially discretized version
of the $q(t,u(x,t))$ (where $q$ is typically the place where spatial
derivatives appear, but this is not of much concern for the moment given that
we only consider time derivatives). In other words, this form fits the general
scheme above if we write 
@f{eqnarray*}
\frac{dy}{dt} = g(t,y) = M^{-1}f(t,y).
@f}

Runke-Kutta methods are time stepping schemes that approximate $y(t_n)\approx
y_{n}$ through a particular one-step approach. They are typically written in the form
@f{eqnarray*}
y_{n+1} = y_n + \sum_{i=1}^s b_i k_i
@f}
where for the form of the right hand side above
@f{eqnarray*}
k_i = h M^{-1} f\left(t_n+c_ih,y_n+\sum_{j=1}^sa_{ij}k_j\right).
@f}
Here $a_{ij}$, $b_i$, and $c_i$ are known coefficients that identify which
particular Runge-Kutta scheme you want to use, and $h=t_{n+1}-t_n$ is the time step
used. Different time stepping methods of the Runge-Kutta class differ in the
number of stages $s$ and the values they use for the coefficients $a_{ij}$,
$b_i$, and $c_i$ but are otherwise easy to implement since one can look up
tabulated values for these coefficients. (These tables are often called
Butcher tableaus.)

At the time of the writing of this tutorial, the methods implemented in
deal.II can be divided in three categories:
<ol>
<li> explicit Runge-Kutta
<li> embedded (or adaptive) Runge-Kutta
<li> implicit Runge-Kutta
</ol> 
Many well known time stepping schemes that one does not typically associated
with the names Runge or Kutta can in fact be written in a way so that they,
too, can be expressed in these categories. They oftentimes represent the
lowest-order members of these families.


<h4>Explicit Runge-Kutta methods</h4> 

These methods, only require a function to evaluate $M^{-1}f(t,y)$ but not
(as implicit methods) to solve an equation that involves
$f(t,y)$ for $y$. As all explicit time stepping methods, they become unstable
when the time step chosen is too large.

Well known methods in this class include forward Euler, third order
Runge-Kutta, and fourth order Runge-Kutta (often abbreviated as RK4).


<h4>Embedded Runge-Kutta methods</h4>

These methods use both a lower and a higher order method to
estimate the error and decide if the time step needs to be shortened or can be
increased. The term "embedded" refers to the fact that the lower-order method
does not require additional evaluates of the function $M^{-1}f(\cdot,\cdot)$
but reuses data that has to be computed for the high order method anyway. It
is, in other words, essentially free, and we get the error estimate as a side
product of using the higher order method.

This class of methods include Heun-Euler, Bogacki-Shampine, Dormand-Prince (ode45 in
Matlab and often abbreviated as RK45 to indicate that the lower and higher order methods
used here are 4th and 5th order Runge-Kutta methods, respectively), Fehlberg,
and Cash-Karp.
 
At the time of the writing, only embedded explicit methods have been implemented.


<h4>Implicit Runge-Kutta methods</h4>

Implicit methods require the solution of (possibly nonlinear) systems of the
form $\alpha y = f(t,y)$
for $y$ in each (sub-)timestep. Internally, this is
done using a Newton-type method and, consequently, they require that the user
provide functions that can evaluate $M^{-1}f(t,y)$ and
$\left(I-\Delta t M^{-1} \frac{\partial f}{\partial y}\right)^{-1}$ or equivalently 
$\left(M - \Delta t \frac{\partial f}{\partial y}\right)^{-1} M$. 

The particular form of this operator results from the fact that each Newton
step requires the solution of an equation of the form
@f{align*}
  \left(M - \Delta t \frac{\partial f}{\partial y}\right) \Delta y
  = -M h(t,y)
@f}
for some (given) $h(t,y)$. Implicit methods are 
always stable, regardless of the time step size, but too large time steps of
course affect the <i>accuracy</i> of the solution, even if the numerical
solution remains stable and bounded.

Methods in this class include backward Euler, implicit midpoint,
Crank-Nicolson, and a two stage SDIRK. 


<h3>Spatially discrete formulation</h3>

By expanding the solution as always using shape functions $\psi_j$ and writing
@f{eqnarray*}
\phi_h(x,t) = \sum_j U_j(t) \psi_j(x),
@f}
we immediately get the spatially discretized version of the diffusion equation as
@f{eqnarray*}
  M \frac{dU(t)}{dt}
  = -{\cal D} U(t) - {\cal A} U(t) + {\cal S}(t)
@f}
where
@f{eqnarray*}
  M_{ij}  &=& (\psi_i,\psi_j), \\
  {\cal D}_{ij}  &=& (D\nabla\psi_i,\nabla\psi_j)_\Omega, \\
  {\cal A}_{ij}  &=& (\Sigma_a\psi_i,\psi_j)_\Omega, \\
  {\cal S}_{i}(t)  &=& (\psi_i,S(x,t))_\Omega.
@f}
See also step-24 and step-26 to understand how we arrive here.
%Boundary terms are not necessary due to the chosen boundary conditions for
the current problem. To use the Runge-Kutta methods, we can then recast this
as follows:
@f{eqnarray*}
f(y) = -{\cal D}y - {\cal A}y + {\cal S}
@f}
In the code, we will need to be able to evaluate this function $f(U)$ along
with its derivative.


<h3>Notes on the testcase</h3>

To simplify the problem, the domain is two dimensional and the mesh is
uniformly refined (there is no need to adapt the mesh since we use quadratic
finite elements and the exact solution is quadratic). Going from a two
dimensional domain to a three dimensional domain is not very
challenging. However if you intend to solve more complex problems where the
mesh must be adapted (as is done, for example, in step-26), then it is
important to remember the following issues:

<ol>
<li> You will need to project the solution to the new mesh when the mesh is changed. Of course,
     the mesh 
     used should be the same at the beginning and at the end of the time step,
     a question that arises because Runge-Kutta methods use multiple
     evaluations of the equations within each time step.
<li> You will need to update the mass matrix and its inverse every time the
     mesh is changed.
</ol>
The techniques for these steps are readily available by looking at step-26.
