
<i>
This program was contributed by Katharina Kormann and Martin
Kronbichler.

The algorithm for the matrix-vector product is based on the article <a
href="http://dx.doi.org/10.1016/j.compfluid.2012.04.012">A generic
interface for parallel cell-based finite element operator
application</a> by Martin Kronbichler and Katharina Kormann, Computers
and Fluids 63:135&ndash;147, 2012, and the paper &quot;Parallel finite element operator
application: Graph partitioning and coloring&quot; by Katharina
Kormann and Martin Kronbichler in: Proceedings of the 7th IEEE
International Conference on e-Science, 2011.  </i>

<a name="Intro"></a>
<h1>Introduction</h1>

This program demonstrates how to use the cell-based implementation of finite
element operators with the MatrixFree class, first introduced in step-37, to
solve nonlinear partial differential equations. Moreover, we demonstrate how
the MatrixFree class handles constraints, an issue shortly mentioned in the
results section of step-37. Finally, we will use an explicit time-stepping
method to solve the problem and introduce Gauss-Lobatto finite elements that
are very convenient in this case since their mass matrix can be accurately
approximated by a diagonal, and thus trivially invertible, matrix. The two
ingredients to this property are firstly a distribution of the nodal points of
Lagrange polynomials according to the point distribution of the Gauss-Lobatto
quadrature rule. Secondly, the quadrature is done with the same Gauss-Lobatto
quadrature rule. In this formula, the integrals $\int_K \varphi_i \varphi_j
dx\approx \sum_q \varphi_i \varphi_j \mathrm{det}(J) \big |_{x_q}$ are
approximated to zero whenever $i\neq j$, because on the points defining the
Lagrange polynomials exactly one function $\varphi_j$ is one and all others
zero. Moreover, the Gauss-Lobatto distribution of nodes of Lagrange
polynomials clusters the nodes towards the element boundaries. This results in
a well-conditioned polynomial basis for high-order discretization
methods. Indeed, the condition number of an FE_Q elements with equidistant
nodes grows exponentially with the degree, which destroys any benefit for
orders of about five and higher. For this reason, Gauss-Lobatto points are the
default distribution for FE_Q (but at degrees one and two, those are
equivalent to the equidistant points).

<h3> Problem statement and discretization </h3>

As an example, we choose to solve the sine-Gordon soliton equation
\f{eqnarray*}
u_{tt} &=& \Delta u -\sin(u) \quad\mbox{for}\quad (x,t) \in
\Omega \times (t_0,t_f],\\
{\mathbf n} \cdot \nabla u &=& 0
\quad\mbox{for}\quad (x,t) \in \partial\Omega \times (t_0,t_f],\\
u(x,t_0) &=& u_0(x).
\f}

that was already introduced in step-25. As a simple explicit time
integration method, we choose leap frog scheme using the second-order
formulation of the equation. Then, the scheme reads in weak form

\f{eqnarray*}
(v,u^{n+1}) = (v,2 u^n-u^{n-1} -
(\Delta t)^2 \sin(u^n)) - (\nabla v, (\Delta t)^2 \nabla u^n),
\f}
where <i> v</i> denotes a test function and the index <i>n</i> stands for
the time step number.

For the spatial discretization, we choose FE_Q elements
with basis functions defined to interpolate the support points of the
Gauss-Lobatto quadrature rule. Moreover, when we compute the integrals
over the basis functions to form the mass matrix and the operator on
the right hand side of the equation above, we use the
Gauss-Lobatto quadrature rule with the same support points as the
node points of the finite element to evaluate the integrals. Since the
finite element is Lagrangian, this will yield a diagonal mass matrix
on the left hand side of the equation, making the solution of the
linear system in each time step trivial.

Using this quadrature rule, for a <i>p</i>th order finite element, we
use a <i>(2p-1)</i>th order accurate formula to evaluate the
integrals. Since the product of two <i>p</i>th order basis functions
when computing a mass matrix gives a function with polynomial degree
<i>2p</i> in each direction, the integrals are not computed exactly.
However, considering the fact that the interpolation order
of finite elements of degree <i>p</i> is <i>p+1</i>, the overall
convergence properties are not disturbed by the quadrature error, in
particular not when we use high orders.

Apart from the fact that we avoid solving linear systems with this
type of elements when using explicit time-stepping, they come with two
other advantages. When we are using the sum-factorization approach to
evaluate the finite element operator (cf. step-37), we have to
evaluate the function at the quadrature points. In the case of
Gauss-Lobatto elements, where quadrature points and node points of the
finite element coincide, this operation is trivial since the value
of the function at the quadrature points is given by its one-dimensional
coefficients. In this way, the complexity of a finite element operator
evaluation is further reduced compared to equidistant elements.

The third advantage is the fact that these elements are better conditioned than
equidistant Lagrange polynomials for increasing order so that we can use
higher order elements for an accurate solution of the equation. Lagrange
elements FE_Q with equidistant points should not be used for polynomial
degrees four and higher.

To sum up the discussion, by using the right finite element and
quadrature rule combination, we end up with a scheme where we in each
time step need to compute the right hand side vector corresponding
to the formulation above and then multiply it by the inverse of the
diagonal mass matrix. In practice, of course, we extract the diagonal
elements and invert them only once at the beginning of the program.

<h3>Implementation of constraints</h3>

The usual way to handle constraints in <code>deal.II</code> is to use
the ConstraintMatrix class that builds a sparse matrix storing
information about which degrees of freedom (DoF) are constrained and
how they are constrained. This format uses an unnecessarily large
amount of memory since there are not so many different types of
constraints: for example, in the case of hanging nodes when using
linear finite element on every cell, constraints most have the form
$x_k = \frac 12 x_i + \frac 12 x_j$ where the coefficients $\frac 12$
are always the same and only $i,j,k$ are different. While storing this
redundant information is not a problem in general because it is only
needed once during matrix and right hand side assembly, it becomes a
problem when we want to use the matrix-free approach since there this
information has to be accessed every time we apply the operator. Thus,
instead of a ConstraintMatrix, we use a variable that we call
<code>constraint_pool</code> that collects the weights of the
different constraints. Then, we only have to store an identifier of
each constraint in the mesh instead of all the weights. Moreover, we
do not want to apply the constraints in a pre- and postprocessing step
but want to take care of constraints as we evaluate the finite element
operator. Therefore, we embed the constraint information into the
variable <code>indices_local_to_global</code> that is used to extract
the cell information from the global vector. If a DoF is constrained,
the <code>indices_local_to_global</code> variable contains the global
indices of the DoFs that it is constrained to. Then, we have another
variable <code>constraint_indicator</code> at hand that holds, for
each cell, the local indices of DoFs that are constrained as well as
the identifier of the type of constraint. Actually, you will not see
these data structures in the example program since the class
<code>FEEvaluation</code> takes care of the constraints without user
interaction.


<h3> Parallelization </h3>

The MatrixFree class comes with the option to be parallelized on three levels:
MPI parallelization on clusters of distributed nodes, thread parallelization
scheduled by the Threading Building Blocks library, and finally with a
vectorization by clustering of two (or more) cells into a SIMD data type for
the operator application. As we have already discussed in step-37, you will
get best performance by using an instruction set specific to your system,
e.g. with the cmake variable <tt>-DCMAKE_CXX_FLAGS="-march=native"</tt>. The
MPI parallelization was already exploited in step-37. Here, we additionally
consider thread parallelization with TBB. This is fairly simple, as all we
need to do is to tell the initialization of the MatrixFree object about the
fact that we want to use a thread parallel scheme through the variable
MatrixFree::AdditionalData::thread_parallel_scheme. During setup, a dependency
graph similar to the one described in the @ref workstream_paper , which allows
the code to schedule the work of the @p local_apply function on chunks of cell
without several threads accessing the same vector indices. As opposed to the
WorkStream loops, some additional clever tricks to avoid global
synchronizations as described in <a
href="https://dx.doi.org/10.1109/eScience.2011.53">Kormann and Kronbichler
(2011)</a> are also applied.

Note that this program is designed to be run with a distributed triangulation
(parallel::distributed::Triangulation), which requires deal.II to be
configured with <a href="http://www.p4est.org/">p4est</a> as described
in the <a href="../../readme.html">deal.II ReadMe</a> file. However, a
non-distributed triangulation is also supported, in which case the
computation will be run in serial.

<h3> The test case </h3>

In our example, we choose the initial value to be \f{eqnarray*} u(x,t) =
\prod_{i=1}^{d} -4 \arctan \left(
\frac{m}{\sqrt{1-m^2}}\frac{\sin\left(\sqrt{1-m^2} t +c_2\right)}{\cosh(mx_i+c_1)}\right)
\f} and solve the equation over the time interval [-10,10]. The
constants are chosen to be $c_1=c_1=0$ and <i> m=0.5</i>. As mentioned
in step-25, in one dimension <i>u</i> as a function of <i>t</i> is the exact
solution of the sine-Gordon equation. For higher dimension, this is however
not the case.
